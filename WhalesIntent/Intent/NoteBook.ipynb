{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce14ee3e",
   "metadata": {},
   "source": [
    "### ETH Whale Activity ML Pipeline\n",
    "\n",
    "- Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6e9c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ imbalanced-learn not installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix, make_scorer\n",
    ")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    HAS_IMBLEARN = True\n",
    "except ImportError:\n",
    "    HAS_IMBLEARN = False\n",
    "    print(\"âš ï¸ imbalanced-learn not installed\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"âš ï¸ XGBoost not installed\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6646b",
   "metadata": {},
   "source": [
    "- Loading and Configuring Environmental Varriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc97b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_WHALES_API\")\n",
    "COINGECKO_API_KEY = os.getenv(\"COINGECKO_API_KEY\")\n",
    "os.makedirs(\"data/price_cache\", exist_ok=True)\n",
    "\n",
    "QUERIES = {\n",
    "    \"whales\": (\"6395391\", \"dune_whales_cache.json\", \"whale_ml_ready.csv\"),\n",
    "    \"market_intent\": (\"6385600\", \"dune_intent_cache.json\", \"market_intent_ml_ready.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef704de4",
   "metadata": {},
   "source": [
    "#### Data Loading Pipeline - Dune + CoinGecko\n",
    "- Data Collection - Fetch Whale Data from Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722ba44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_dune(qid, cache):\n",
    "    headers = {\"x-dune-api-key\": DUNE_API_KEY}\n",
    "    today = pd.Timestamp.now(tz='UTC').normalize()\n",
    "    \n",
    "    # Load cache\n",
    "    if os.path.exists(cache):\n",
    "        c = json.load(open(cache))\n",
    "        df_cached = pd.DataFrame(c[\"data\"])\n",
    "        df_cached[\"block_date\"] = pd.to_datetime(df_cached[\"block_date\"], utc=True)\n",
    "        last_date = pd.to_datetime(c[\"last_block_date\"], utc=True)\n",
    "        \n",
    "        # If cache has yesterday's data, it's current enough (today's data doesn't exist yet)\n",
    "        if last_date >= today - timedelta(1):\n",
    "            print(f\"âœ… {cache} current ({last_date.date()})\")\n",
    "            return df_cached\n",
    "        \n",
    "        print(f\"ðŸ”„ {cache}: fetching latest\")\n",
    "    else:\n",
    "        df_cached = pd.DataFrame()\n",
    "        print(f\"ðŸ†• {cache}: full fetch\")\n",
    "    \n",
    "    # Execute query\n",
    "    resp = requests.post(f\"https://api.dune.com/api/v1/query/{qid}/execute\", headers=headers).json()\n",
    "    if \"execution_id\" not in resp:\n",
    "        raise RuntimeError(f\"Dune API error: {resp}\")\n",
    "    eid = resp[\"execution_id\"]\n",
    "    \n",
    "    # Poll\n",
    "    while True:\n",
    "        s = requests.get(f\"https://api.dune.com/api/v1/execution/{eid}/status\", headers=headers).json()[\"state\"]\n",
    "        if s == \"QUERY_STATE_COMPLETED\": break\n",
    "        if s == \"QUERY_STATE_FAILED\": raise RuntimeError(\"Query failed\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Get results & merge with cache\n",
    "    df_new = pd.DataFrame(requests.get(f\"https://api.dune.com/api/v1/execution/{eid}/results\", headers=headers).json()[\"result\"][\"rows\"])\n",
    "    if df_new.empty: return df_cached\n",
    "    \n",
    "    df_new[\"block_date\"] = pd.to_datetime(df_new[\"block_date\"], utc=True)\n",
    "    df = pd.concat([df_cached, df_new[df_new[\"block_date\"] < today]]).drop_duplicates(\"block_date\", keep=\"last\").sort_values(\"block_date\").reset_index(drop=True)\n",
    "    \n",
    "    # Save cache\n",
    "    json.dump({\"last_block_date\": df[\"block_date\"].max().strftime(\"%Y-%m-%d\"), \n",
    "               \"data\": json.loads(df.to_json(orient=\"records\", date_format=\"iso\"))}, open(cache, \"w\"))\n",
    "    print(f\"âœ… {cache}: {len(df)} rows (added {len(df_new)} new)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e9ebf",
   "metadata": {},
   "source": [
    "- Load Dune Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6edaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================ \n",
      "DUNE DATA\n",
      " ============================================================\n",
      "âœ… dune_whales_cache.json current (2025-12-24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dune_intent_cache.json current (2025-12-24)\n",
      "\n",
      "âœ… Whales: 1097 | Intent: 1097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*60, \"\\nDUNE DATA\\n\", \"=\"*60)\n",
    "datasets = {}\n",
    "for name, (qid, cache, output) in QUERIES.items():\n",
    "    datasets[name] = fetch_dune(qid, cache)\n",
    "    datasets[name].to_csv(output, index=False)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "df_whales, df_market_intent = datasets[\"whales\"], datasets[\"market_intent\"]\n",
    "print(f\"\\nâœ… Whales: {len(df_whales)} | Intent: {len(df_market_intent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce1ff5",
   "metadata": {},
   "source": [
    "- Coingecko Price Function\n",
    "     - Utilities:Normalize any date-like input to UTC Timestamp.\n",
    "    Handles tz-naive, tz-aware, date, datetime safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf029cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_utc(ts):\n",
    "    ts = pd.Timestamp(ts)\n",
    "    if ts.tzinfo is None:\n",
    "        return ts.tz_localize(\"UTC\")\n",
    "    return ts.tz_convert(\"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b99df6",
   "metadata": {},
   "source": [
    "- CoinGecko chunked fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25321ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cg_chunked(cg_id, start, end, key=None, days=30):\n",
    "    \"\"\"\n",
    "    Fetch DAILY UTC prices from CoinGecko.\n",
    "    No skipped days. No today.\n",
    "    \"\"\"\n",
    "    url_base = \"https://pro-api.coingecko.com/api/v3\" if key else \"https://api.coingecko.com/api/v3\"\n",
    "    headers = {\"x-cg-pro-api-key\": key} if key else {}\n",
    "\n",
    "    start_dt = to_utc(start)\n",
    "    end_dt   = to_utc(end) + pd.Timedelta(days=1)  # âœ… INCLUSIVE FIX\n",
    "\n",
    "    all_prices = []\n",
    "    curr = start_dt\n",
    "\n",
    "    while curr < end_dt:\n",
    "        next_dt = min(curr + pd.Timedelta(days=days), end_dt)\n",
    "\n",
    "        params = {\n",
    "            \"vs_currency\": \"usd\",\n",
    "            \"from\": int(curr.timestamp()),\n",
    "            \"to\": int(next_dt.timestamp())\n",
    "        }\n",
    "\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                r = requests.get(\n",
    "                    f\"{url_base}/coins/{cg_id}/market_chart/range\",\n",
    "                    params=params,\n",
    "                    headers=headers,\n",
    "                    timeout=30\n",
    "                )\n",
    "                r.raise_for_status()\n",
    "\n",
    "                prices = r.json().get(\"prices\", [])\n",
    "                all_prices.extend(prices)\n",
    "\n",
    "                print(f\"ðŸ“¥ {cg_id}: {curr.date()} â†’ {next_dt.date()} ({len(prices)} pts)\")\n",
    "                time.sleep(0.3)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == 2:\n",
    "                    raise\n",
    "                print(f\"âš ï¸ Retry {attempt + 1}/3 ({e})\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        # NO +1 DAY SKIP\n",
    "        curr = next_dt\n",
    "\n",
    "    if not all_prices:\n",
    "        return pd.DataFrame(columns=[\"date\", \"price\"])\n",
    "\n",
    "    # Build DAILY UTC prices\n",
    "    df = pd.DataFrame(all_prices, columns=[\"timestamp\", \"price\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True).dt.floor(\"D\")\n",
    "\n",
    "    df = (\n",
    "        df.groupby(\"date\", as_index=False)[\"price\"]\n",
    "          .mean()\n",
    "          .sort_values(\"date\")\n",
    "    )\n",
    "\n",
    "    # Enforce full daily range (no silent gaps)\n",
    "    \n",
    "    full_range = pd.date_range(\n",
    "        start=df[\"date\"].min(),\n",
    "        end=df[\"date\"].max(),\n",
    "        freq=\"D\",\n",
    "        tz=\"UTC\"\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.set_index(\"date\")\n",
    "          .reindex(full_range)\n",
    "          .rename_axis(\"date\")\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a99f3",
   "metadata": {},
   "source": [
    "- Cached price loader (EXCLUDES TODAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080da28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(sym, cg_id, start, end, key=None):\n",
    "    \"\"\"\n",
    "    Load DAILY prices through YESTERDAY ONLY.\n",
    "    Today is always excluded.\n",
    "    \"\"\"\n",
    "    cache = f\"data/price_cache/{sym}.csv\"\n",
    "\n",
    "    today_utc = pd.Timestamp.utcnow().floor(\"D\")\n",
    "    yesterday = today_utc - pd.Timedelta(days=1)\n",
    "\n",
    "    start = to_utc(start)\n",
    "    end   = min(to_utc(end), yesterday)\n",
    "\n",
    "    if start > end:\n",
    "        return pd.DataFrame(columns=[\"date\", f\"{sym}_price\"])\n",
    "\n",
    "    # Cache exists\n",
    "    \n",
    "    if os.path.exists(cache):\n",
    "        df = pd.read_csv(cache, parse_dates=[\"date\"])\n",
    "        df[\"date\"] = df[\"date\"].apply(to_utc)\n",
    "\n",
    "        last_cached = df[\"date\"].max()\n",
    "\n",
    "        if last_cached >= end:\n",
    "            print(f\"âœ… {sym.upper()} cache current ({last_cached.date()})\")\n",
    "            return df\n",
    "\n",
    "        fetch_start = last_cached + pd.Timedelta(days=1)\n",
    "        print(f\"ðŸ”„ {sym.upper()}: fetching {fetch_start.date()} â†’ {end.date()}\")\n",
    "\n",
    "        new = fetch_cg_chunked(cg_id, fetch_start, end, key)\n",
    "\n",
    "        if not new.empty:\n",
    "            new = new.rename(columns={\"price\": f\"{sym}_price\"})\n",
    "            df = (\n",
    "                pd.concat([df, new])\n",
    "                  .drop_duplicates(\"date\", keep=\"last\")\n",
    "                  .sort_values(\"date\")\n",
    "                  .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "       # No cache\n",
    "   \n",
    "    else:\n",
    "        print(f\"ðŸ“¦ {sym.upper()}: full fetch {start.date()} â†’ {end.date()}\")\n",
    "        df = fetch_cg_chunked(cg_id, start, end, key)\n",
    "\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        df = df.rename(columns={\"price\": f\"{sym}_price\"})\n",
    "\n",
    "    df.to_csv(cache, index=False)\n",
    "    print(f\"âœ… {sym.upper()} saved (through {df['date'].max().date()})\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7542788",
   "metadata": {},
   "source": [
    "- Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a89a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“… Range: 2022-09-15 â†’ 2025-12-24 (today excluded)\n",
      "\n",
      "âœ… BTC cache current (2025-12-24)\n",
      "âœ… ETH cache current (2025-12-24)\n",
      "\n",
      "âœ… FINAL CHECK\n",
      "BTC last date: 2025-12-24\n",
      "ETH last date: 2025-12-24\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    min_date = min(\n",
    "        df_whales[\"block_date\"].min(),\n",
    "        df_market_intent[\"block_date\"].min()\n",
    "    ) - pd.Timedelta(days=100)\n",
    "\n",
    "    max_date = max(\n",
    "        df_whales[\"block_date\"].max(),\n",
    "        df_market_intent[\"block_date\"].max()\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸ“… Range: {min_date.date()} â†’ {max_date.date()} (today excluded)\\n\")\n",
    "\n",
    "    df_btc = get_price(\"btc\", \"bitcoin\", min_date, max_date, COINGECKO_API_KEY)\n",
    "    df_eth = get_price(\"eth\", \"ethereum\", min_date, max_date, COINGECKO_API_KEY)\n",
    "\n",
    "    print(\"\\nâœ… FINAL CHECK\")\n",
    "    print(f\"BTC last date: {df_btc['date'].max().date()}\")\n",
    "    print(f\"ETH last date: {df_eth['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc579210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_date</th>\n",
       "      <th>block_fullness_delta_1d</th>\n",
       "      <th>eth_burned_delta_1d</th>\n",
       "      <th>eth_burned_zscore_90d</th>\n",
       "      <th>exchange_flow_share</th>\n",
       "      <th>median_gas_delta_1d</th>\n",
       "      <th>median_gas_delta_7d</th>\n",
       "      <th>net_exchange_flow_ratio</th>\n",
       "      <th>smart_contract_ratio_delta_1d</th>\n",
       "      <th>tx_per_active_delta_1d</th>\n",
       "      <th>tx_per_active_zscore_90d</th>\n",
       "      <th>whale_exchange_asymmetry</th>\n",
       "      <th>whale_exchange_flow_ratio</th>\n",
       "      <th>whale_tx_zscore_90d</th>\n",
       "      <th>whale_volume_ratio</th>\n",
       "      <th>whale_volume_ratio_delta_1d</th>\n",
       "      <th>whale_volume_ratio_delta_3d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-24 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.214255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.220737</td>\n",
       "      <td>-0.041071</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.787476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.465847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.067329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.145227</td>\n",
       "      <td>-0.064300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.828582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.267112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.274527</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.802250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-27 00:00:00+00:00</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>358.3551</td>\n",
       "      <td>0.7071</td>\n",
       "      <td>0.232362</td>\n",
       "      <td>3.0361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.070792</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.7071</td>\n",
       "      <td>-0.312723</td>\n",
       "      <td>-0.063788</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.801580</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-28 00:00:00+00:00</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>120.1106</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>0.241721</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.081152</td>\n",
       "      <td>0.014817</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.8405</td>\n",
       "      <td>-0.351500</td>\n",
       "      <td>-0.077298</td>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.836426</td>\n",
       "      <td>0.034846</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2025-12-20 00:00:00+00:00</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>-22.1130</td>\n",
       "      <td>-0.3346</td>\n",
       "      <td>0.203059</td>\n",
       "      <td>-0.1076</td>\n",
       "      <td>-0.0303</td>\n",
       "      <td>-0.051241</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>-1.2810</td>\n",
       "      <td>-0.238883</td>\n",
       "      <td>-0.046222</td>\n",
       "      <td>-1.8828</td>\n",
       "      <td>0.851792</td>\n",
       "      <td>-0.103314</td>\n",
       "      <td>-0.107734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2025-12-21 00:00:00+00:00</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>-0.3298</td>\n",
       "      <td>0.354921</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>-0.0284</td>\n",
       "      <td>0.055381</td>\n",
       "      <td>0.057881</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.9035</td>\n",
       "      <td>0.171106</td>\n",
       "      <td>0.059485</td>\n",
       "      <td>-2.4013</td>\n",
       "      <td>0.889958</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>-0.052551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2025-12-22 00:00:00+00:00</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>1.3817</td>\n",
       "      <td>-0.3228</td>\n",
       "      <td>0.301284</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>-0.066520</td>\n",
       "      <td>-0.066476</td>\n",
       "      <td>-0.4239</td>\n",
       "      <td>-1.5519</td>\n",
       "      <td>-0.216851</td>\n",
       "      <td>-0.064440</td>\n",
       "      <td>0.7183</td>\n",
       "      <td>0.936694</td>\n",
       "      <td>0.046736</td>\n",
       "      <td>-0.018412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2025-12-23 00:00:00+00:00</td>\n",
       "      <td>-0.000778</td>\n",
       "      <td>1.5696</td>\n",
       "      <td>-0.3154</td>\n",
       "      <td>0.224627</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.029405</td>\n",
       "      <td>0.041258</td>\n",
       "      <td>-0.1504</td>\n",
       "      <td>-2.6310</td>\n",
       "      <td>-0.127150</td>\n",
       "      <td>-0.028212</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.104255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2025-12-24 00:00:00+00:00</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-1.4847</td>\n",
       "      <td>-0.3121</td>\n",
       "      <td>0.315670</td>\n",
       "      <td>-0.0143</td>\n",
       "      <td>-0.0550</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.038949</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>-1.7725</td>\n",
       "      <td>-0.398989</td>\n",
       "      <td>-0.124283</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>0.940054</td>\n",
       "      <td>-0.015993</td>\n",
       "      <td>0.050096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     block_date  block_fullness_delta_1d  eth_burned_delta_1d  \\\n",
       "0     2022-12-24 00:00:00+00:00                      NaN                  NaN   \n",
       "1     2022-12-25 00:00:00+00:00                      NaN                  NaN   \n",
       "2     2022-12-26 00:00:00+00:00                      NaN                  NaN   \n",
       "3     2022-12-27 00:00:00+00:00                -0.000582             358.3551   \n",
       "4     2022-12-28 00:00:00+00:00                -0.000340             120.1106   \n",
       "...                         ...                      ...                  ...   \n",
       "1092  2025-12-20 00:00:00+00:00                 0.001126             -22.1130   \n",
       "1093  2025-12-21 00:00:00+00:00                 0.000199               0.4850   \n",
       "1094  2025-12-22 00:00:00+00:00                -0.001665               1.3817   \n",
       "1095  2025-12-23 00:00:00+00:00                -0.000778               1.5696   \n",
       "1096  2025-12-24 00:00:00+00:00                -0.000190              -1.4847   \n",
       "\n",
       "      eth_burned_zscore_90d  exchange_flow_share  median_gas_delta_1d  \\\n",
       "0                    0.0000             0.214255                  NaN   \n",
       "1                    0.0000             0.465847                  NaN   \n",
       "2                    0.0000             0.267112                  NaN   \n",
       "3                    0.7071             0.232362               3.0361   \n",
       "4                    0.8016             0.241721               0.6857   \n",
       "...                     ...                  ...                  ...   \n",
       "1092                -0.3346             0.203059              -0.1076   \n",
       "1093                -0.3298             0.354921               0.0038   \n",
       "1094                -0.3228             0.301284               0.0098   \n",
       "1095                -0.3154             0.224627               0.0101   \n",
       "1096                -0.3121             0.315670              -0.0143   \n",
       "\n",
       "      median_gas_delta_7d  net_exchange_flow_ratio  \\\n",
       "0                     NaN                -0.046363   \n",
       "1                     NaN                -0.067329   \n",
       "2                     NaN                 0.059721   \n",
       "3                     NaN                -0.070792   \n",
       "4                     NaN                -0.081152   \n",
       "...                   ...                      ...   \n",
       "1092              -0.0303                -0.051241   \n",
       "1093              -0.0284                 0.055381   \n",
       "1094              -0.1075                -0.066520   \n",
       "1095              -0.0248                -0.029405   \n",
       "1096              -0.0550                -0.126877   \n",
       "\n",
       "      smart_contract_ratio_delta_1d  tx_per_active_delta_1d  \\\n",
       "0                               NaN                     NaN   \n",
       "1                               NaN                     NaN   \n",
       "2                               NaN                     NaN   \n",
       "3                         -0.001002                  0.0820   \n",
       "4                          0.014817                  0.0344   \n",
       "...                             ...                     ...   \n",
       "1092                       0.013453                 -0.1327   \n",
       "1093                       0.057881                  0.3857   \n",
       "1094                      -0.066476                 -0.4239   \n",
       "1095                       0.041258                 -0.1504   \n",
       "1096                      -0.038949                  0.1057   \n",
       "\n",
       "      tx_per_active_zscore_90d  whale_exchange_asymmetry  \\\n",
       "0                       0.0000                 -0.220737   \n",
       "1                       0.0000                 -0.145227   \n",
       "2                       0.0000                  0.274527   \n",
       "3                       0.7071                 -0.312723   \n",
       "4                       0.8405                 -0.351500   \n",
       "...                        ...                       ...   \n",
       "1092                   -1.2810                 -0.238883   \n",
       "1093                    1.9035                  0.171106   \n",
       "1094                   -1.5519                 -0.216851   \n",
       "1095                   -2.6310                 -0.127150   \n",
       "1096                   -1.7725                 -0.398989   \n",
       "\n",
       "      whale_exchange_flow_ratio  whale_tx_zscore_90d  whale_volume_ratio  \\\n",
       "0                     -0.041071               0.0000            0.787476   \n",
       "1                     -0.064300               0.0000            0.828582   \n",
       "2                      0.065927               0.0000            0.802250   \n",
       "3                     -0.063788               0.0555            0.801580   \n",
       "4                     -0.077298               0.9225            0.836426   \n",
       "...                         ...                  ...                 ...   \n",
       "1092                  -0.046222              -1.8828            0.851792   \n",
       "1093                   0.059485              -2.4013            0.889958   \n",
       "1094                  -0.064440               0.7183            0.936694   \n",
       "1095                  -0.028212               0.4125            0.956047   \n",
       "1096                  -0.124283               0.4733            0.940054   \n",
       "\n",
       "      whale_volume_ratio_delta_1d  whale_volume_ratio_delta_3d  \n",
       "0                             NaN                          NaN  \n",
       "1                             NaN                          NaN  \n",
       "2                             NaN                          NaN  \n",
       "3                       -0.000671                          NaN  \n",
       "4                        0.034846                          NaN  \n",
       "...                           ...                          ...  \n",
       "1092                    -0.103314                    -0.107734  \n",
       "1093                     0.038166                    -0.052551  \n",
       "1094                     0.046736                    -0.018412  \n",
       "1095                     0.019353                     0.104255  \n",
       "1096                    -0.015993                     0.050096  \n",
       "\n",
       "[1097 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/workspaces/Whale-Movement-Based-Price-Direction-Generator-V2/WhalesIntent/Intent/market_intent_ml_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a50ddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_date</th>\n",
       "      <th>deposit_tx_count</th>\n",
       "      <th>deposit_withdrawal_ratio</th>\n",
       "      <th>exchange_volume_ratio</th>\n",
       "      <th>mega_whale_ratio</th>\n",
       "      <th>mega_whale_tx_count</th>\n",
       "      <th>mega_whale_volume_eth</th>\n",
       "      <th>net_flow_ma7</th>\n",
       "      <th>non_exchange_ratio</th>\n",
       "      <th>non_exchange_tx_count</th>\n",
       "      <th>non_exchange_volume_eth</th>\n",
       "      <th>std_whale_tx_size_eth</th>\n",
       "      <th>whale_exchange_deposits_eth</th>\n",
       "      <th>whale_exchange_withdrawals_eth</th>\n",
       "      <th>whale_net_exchange_flow_eth</th>\n",
       "      <th>whale_tx_count</th>\n",
       "      <th>whale_volume_eth</th>\n",
       "      <th>withdrawal_tx_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-24 00:00:00+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.5554</td>\n",
       "      <td>0.2899</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>36</td>\n",
       "      <td>1.083547e+05</td>\n",
       "      <td>-7203.9190</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>24</td>\n",
       "      <td>81206.9408</td>\n",
       "      <td>3279.562931</td>\n",
       "      <td>20175.0838</td>\n",
       "      <td>12971.1648</td>\n",
       "      <td>-7203.9190</td>\n",
       "      <td>42</td>\n",
       "      <td>1.143532e+05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 00:00:00+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>4.4874</td>\n",
       "      <td>0.6327</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>36</td>\n",
       "      <td>1.475401e+05</td>\n",
       "      <td>-59329.7899</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>18</td>\n",
       "      <td>54184.9701</td>\n",
       "      <td>8106.493990</td>\n",
       "      <td>76342.4799</td>\n",
       "      <td>17012.6900</td>\n",
       "      <td>-59329.7899</td>\n",
       "      <td>36</td>\n",
       "      <td>1.475401e+05</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.3896</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>50</td>\n",
       "      <td>1.961529e+05</td>\n",
       "      <td>53454.7638</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>38</td>\n",
       "      <td>122784.2329</td>\n",
       "      <td>7771.982946</td>\n",
       "      <td>12456.6605</td>\n",
       "      <td>65911.4244</td>\n",
       "      <td>53454.7638</td>\n",
       "      <td>55</td>\n",
       "      <td>2.011523e+05</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-27 00:00:00+00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0556</td>\n",
       "      <td>0.2697</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>74</td>\n",
       "      <td>2.223786e+05</td>\n",
       "      <td>16368.8473</td>\n",
       "      <td>0.7303</td>\n",
       "      <td>45</td>\n",
       "      <td>162409.1610</td>\n",
       "      <td>3078.704806</td>\n",
       "      <td>40343.2672</td>\n",
       "      <td>19626.1980</td>\n",
       "      <td>-20717.0692</td>\n",
       "      <td>74</td>\n",
       "      <td>2.223786e+05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-28 00:00:00+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>6.4245</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>65</td>\n",
       "      <td>3.348447e+05</td>\n",
       "      <td>-8432.1986</td>\n",
       "      <td>0.7628</td>\n",
       "      <td>49</td>\n",
       "      <td>255413.2942</td>\n",
       "      <td>6903.329720</td>\n",
       "      <td>68732.8362</td>\n",
       "      <td>10698.5457</td>\n",
       "      <td>-58034.2905</td>\n",
       "      <td>65</td>\n",
       "      <td>3.348447e+05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2025-12-20 00:00:00+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0293</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.632716e+05</td>\n",
       "      <td>-91552.7172</td>\n",
       "      <td>0.7358</td>\n",
       "      <td>29</td>\n",
       "      <td>120134.5612</td>\n",
       "      <td>3108.900831</td>\n",
       "      <td>39550.9925</td>\n",
       "      <td>3586.0000</td>\n",
       "      <td>-35964.9925</td>\n",
       "      <td>40</td>\n",
       "      <td>1.632716e+05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2025-12-21 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>32</td>\n",
       "      <td>1.928101e+05</td>\n",
       "      <td>-100668.3469</td>\n",
       "      <td>0.4752</td>\n",
       "      <td>19</td>\n",
       "      <td>91619.3550</td>\n",
       "      <td>4433.349204</td>\n",
       "      <td>47347.1628</td>\n",
       "      <td>53843.5845</td>\n",
       "      <td>6496.4217</td>\n",
       "      <td>32</td>\n",
       "      <td>1.928101e+05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2025-12-22 00:00:00+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>2.7336</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>70</td>\n",
       "      <td>6.105957e+05</td>\n",
       "      <td>-95628.5165</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>51</td>\n",
       "      <td>393401.9015</td>\n",
       "      <td>5992.049099</td>\n",
       "      <td>159021.1937</td>\n",
       "      <td>58172.5992</td>\n",
       "      <td>-100848.5945</td>\n",
       "      <td>70</td>\n",
       "      <td>6.105957e+05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2025-12-23 00:00:00+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>0.2043</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>69</td>\n",
       "      <td>1.014827e+06</td>\n",
       "      <td>-72486.9484</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>53</td>\n",
       "      <td>807479.5955</td>\n",
       "      <td>16471.118552</td>\n",
       "      <td>127968.2705</td>\n",
       "      <td>79379.5928</td>\n",
       "      <td>-48588.6777</td>\n",
       "      <td>69</td>\n",
       "      <td>1.014827e+06</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2025-12-24 00:00:00+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>3.6162</td>\n",
       "      <td>0.4103</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>57</td>\n",
       "      <td>6.367684e+05</td>\n",
       "      <td>-85556.8204</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>40</td>\n",
       "      <td>375498.8864</td>\n",
       "      <td>17138.692766</td>\n",
       "      <td>204671.6325</td>\n",
       "      <td>56597.8625</td>\n",
       "      <td>-148073.7700</td>\n",
       "      <td>57</td>\n",
       "      <td>6.367684e+05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     block_date  deposit_tx_count  deposit_withdrawal_ratio  \\\n",
       "0     2022-12-24 00:00:00+00:00                11                    1.5554   \n",
       "1     2022-12-25 00:00:00+00:00                10                    4.4874   \n",
       "2     2022-12-26 00:00:00+00:00                 8                    0.1890   \n",
       "3     2022-12-27 00:00:00+00:00                19                    2.0556   \n",
       "4     2022-12-28 00:00:00+00:00                14                    6.4245   \n",
       "...                         ...               ...                       ...   \n",
       "1092  2025-12-20 00:00:00+00:00                 9                   11.0293   \n",
       "1093  2025-12-21 00:00:00+00:00                 8                    0.8793   \n",
       "1094  2025-12-22 00:00:00+00:00                14                    2.7336   \n",
       "1095  2025-12-23 00:00:00+00:00                11                    1.6121   \n",
       "1096  2025-12-24 00:00:00+00:00                13                    3.6162   \n",
       "\n",
       "      exchange_volume_ratio  mega_whale_ratio  mega_whale_tx_count  \\\n",
       "0                    0.2899            0.9475                   36   \n",
       "1                    0.6327            1.0000                   36   \n",
       "2                    0.3896            0.9751                   50   \n",
       "3                    0.2697            1.0000                   74   \n",
       "4                    0.2372            1.0000                   65   \n",
       "...                     ...               ...                  ...   \n",
       "1092                 0.2642            1.0000                   40   \n",
       "1093                 0.5248            1.0000                   32   \n",
       "1094                 0.3557            1.0000                   70   \n",
       "1095                 0.2043            1.0000                   69   \n",
       "1096                 0.4103            1.0000                   57   \n",
       "\n",
       "      mega_whale_volume_eth  net_flow_ma7  non_exchange_ratio  \\\n",
       "0              1.083547e+05    -7203.9190              0.7101   \n",
       "1              1.475401e+05   -59329.7899              0.3673   \n",
       "2              1.961529e+05    53454.7638              0.6104   \n",
       "3              2.223786e+05    16368.8473              0.7303   \n",
       "4              3.348447e+05    -8432.1986              0.7628   \n",
       "...                     ...           ...                 ...   \n",
       "1092           1.632716e+05   -91552.7172              0.7358   \n",
       "1093           1.928101e+05  -100668.3469              0.4752   \n",
       "1094           6.105957e+05   -95628.5165              0.6443   \n",
       "1095           1.014827e+06   -72486.9484              0.7957   \n",
       "1096           6.367684e+05   -85556.8204              0.5897   \n",
       "\n",
       "      non_exchange_tx_count  non_exchange_volume_eth  std_whale_tx_size_eth  \\\n",
       "0                        24               81206.9408            3279.562931   \n",
       "1                        18               54184.9701            8106.493990   \n",
       "2                        38              122784.2329            7771.982946   \n",
       "3                        45              162409.1610            3078.704806   \n",
       "4                        49              255413.2942            6903.329720   \n",
       "...                     ...                      ...                    ...   \n",
       "1092                     29              120134.5612            3108.900831   \n",
       "1093                     19               91619.3550            4433.349204   \n",
       "1094                     51              393401.9015            5992.049099   \n",
       "1095                     53              807479.5955           16471.118552   \n",
       "1096                     40              375498.8864           17138.692766   \n",
       "\n",
       "      whale_exchange_deposits_eth  whale_exchange_withdrawals_eth  \\\n",
       "0                      20175.0838                      12971.1648   \n",
       "1                      76342.4799                      17012.6900   \n",
       "2                      12456.6605                      65911.4244   \n",
       "3                      40343.2672                      19626.1980   \n",
       "4                      68732.8362                      10698.5457   \n",
       "...                           ...                             ...   \n",
       "1092                   39550.9925                       3586.0000   \n",
       "1093                   47347.1628                      53843.5845   \n",
       "1094                  159021.1937                      58172.5992   \n",
       "1095                  127968.2705                      79379.5928   \n",
       "1096                  204671.6325                      56597.8625   \n",
       "\n",
       "      whale_net_exchange_flow_eth  whale_tx_count  whale_volume_eth  \\\n",
       "0                      -7203.9190              42      1.143532e+05   \n",
       "1                     -59329.7899              36      1.475401e+05   \n",
       "2                      53454.7638              55      2.011523e+05   \n",
       "3                     -20717.0692              74      2.223786e+05   \n",
       "4                     -58034.2905              65      3.348447e+05   \n",
       "...                           ...             ...               ...   \n",
       "1092                  -35964.9925              40      1.632716e+05   \n",
       "1093                    6496.4217              32      1.928101e+05   \n",
       "1094                 -100848.5945              70      6.105957e+05   \n",
       "1095                  -48588.6777              69      1.014827e+06   \n",
       "1096                 -148073.7700              57      6.367684e+05   \n",
       "\n",
       "      withdrawal_tx_count  \n",
       "0                       7  \n",
       "1                       8  \n",
       "2                       9  \n",
       "3                      10  \n",
       "4                       2  \n",
       "...                   ...  \n",
       "1092                    2  \n",
       "1093                    5  \n",
       "1094                    5  \n",
       "1095                    5  \n",
       "1096                    4  \n",
       "\n",
       "[1097 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/workspaces/Whale-Movement-Based-Price-Direction-Generator-V2/WhalesIntent/Intent/whale_ml_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89073c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>btc_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-15 00:00:00+00:00</td>\n",
       "      <td>20009.052983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-16 00:00:00+00:00</td>\n",
       "      <td>19713.657885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-17 00:00:00+00:00</td>\n",
       "      <td>19944.803793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18 00:00:00+00:00</td>\n",
       "      <td>19896.269820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-19 00:00:00+00:00</td>\n",
       "      <td>19028.641104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>2025-12-20 00:00:00+00:00</td>\n",
       "      <td>88174.588995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>2025-12-21 00:00:00+00:00</td>\n",
       "      <td>88260.199788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>2025-12-22 00:00:00+00:00</td>\n",
       "      <td>89102.172598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>2025-12-23 00:00:00+00:00</td>\n",
       "      <td>87769.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>2025-12-24 00:00:00+00:00</td>\n",
       "      <td>87238.985295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1197 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date     btc_price\n",
       "0     2022-09-15 00:00:00+00:00  20009.052983\n",
       "1     2022-09-16 00:00:00+00:00  19713.657885\n",
       "2     2022-09-17 00:00:00+00:00  19944.803793\n",
       "3     2022-09-18 00:00:00+00:00  19896.269820\n",
       "4     2022-09-19 00:00:00+00:00  19028.641104\n",
       "...                         ...           ...\n",
       "1192  2025-12-20 00:00:00+00:00  88174.588995\n",
       "1193  2025-12-21 00:00:00+00:00  88260.199788\n",
       "1194  2025-12-22 00:00:00+00:00  89102.172598\n",
       "1195  2025-12-23 00:00:00+00:00  87769.792000\n",
       "1196  2025-12-24 00:00:00+00:00  87238.985295\n",
       "\n",
       "[1197 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/workspaces/Whale-Movement-Based-Price-Direction-Generator-V2/WhalesIntent/Intent/data/price_cache/btc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637c5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>eth_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-15 00:00:00+00:00</td>\n",
       "      <td>1566.672324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-16 00:00:00+00:00</td>\n",
       "      <td>1456.046704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-17 00:00:00+00:00</td>\n",
       "      <td>1443.421822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18 00:00:00+00:00</td>\n",
       "      <td>1424.248562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-19 00:00:00+00:00</td>\n",
       "      <td>1330.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>2025-12-20 00:00:00+00:00</td>\n",
       "      <td>2979.478368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>2025-12-21 00:00:00+00:00</td>\n",
       "      <td>2980.317996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>2025-12-22 00:00:00+00:00</td>\n",
       "      <td>3025.604874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>2025-12-23 00:00:00+00:00</td>\n",
       "      <td>2966.388075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>2025-12-24 00:00:00+00:00</td>\n",
       "      <td>2936.479989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1197 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date    eth_price\n",
       "0     2022-09-15 00:00:00+00:00  1566.672324\n",
       "1     2022-09-16 00:00:00+00:00  1456.046704\n",
       "2     2022-09-17 00:00:00+00:00  1443.421822\n",
       "3     2022-09-18 00:00:00+00:00  1424.248562\n",
       "4     2022-09-19 00:00:00+00:00  1330.210600\n",
       "...                         ...          ...\n",
       "1192  2025-12-20 00:00:00+00:00  2979.478368\n",
       "1193  2025-12-21 00:00:00+00:00  2980.317996\n",
       "1194  2025-12-22 00:00:00+00:00  3025.604874\n",
       "1195  2025-12-23 00:00:00+00:00  2966.388075\n",
       "1196  2025-12-24 00:00:00+00:00  2936.479989\n",
       "\n",
       "[1197 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/workspaces/Whale-Movement-Based-Price-Direction-Generator-V2/WhalesIntent/Intent/data/price_cache/eth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47296abf",
   "metadata": {},
   "source": [
    "- Merging the Dataset into single Dataset ready for feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fdc414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading datasets...\n",
      "ðŸ“… Converting dates...\n",
      "âœ… Whales: 1097 rows\n",
      "âœ… Market Intent: 1097 rows\n",
      "âœ… BTC Prices: 1197 rows\n",
      "âœ… ETH Prices: 1197 rows\n",
      "\n",
      "ðŸ”— Merging price data...\n",
      "âœ… Combined prices: 1197 rows\n",
      "\n",
      "ðŸ”— Merging whale data with prices...\n",
      "âœ… After whale + prices: 1097 rows\n",
      "\n",
      "ðŸ”— Merging market intent data...\n",
      "âœ… Final merged dataset: 1097 rows\n",
      "\n",
      "ðŸ” Checking data quality...\n",
      "   Missing BTC prices: 0\n",
      "   Missing ETH prices: 0\n",
      "   Missing intent data: 0.0\n",
      "\n",
      "ðŸ“… Date range: 2022-12-24 â†’ 2025-12-24\n",
      "ðŸ“Š Total columns: 36\n",
      "\n",
      "ðŸ’¾ Saved: merged_ml_dataset.csv\n",
      "\n",
      "ðŸ“‹ Sample of merged data:\n",
      "                 block_date  deposit_tx_count  deposit_withdrawal_ratio  \\\n",
      "0 2022-12-24 00:00:00+00:00                11                    1.5554   \n",
      "1 2022-12-25 00:00:00+00:00                10                    4.4874   \n",
      "2 2022-12-26 00:00:00+00:00                 8                    0.1890   \n",
      "3 2022-12-27 00:00:00+00:00                19                    2.0556   \n",
      "4 2022-12-28 00:00:00+00:00                14                    6.4245   \n",
      "\n",
      "   exchange_volume_ratio  mega_whale_ratio  mega_whale_tx_count  \\\n",
      "0                 0.2899            0.9475                   36   \n",
      "1                 0.6327            1.0000                   36   \n",
      "2                 0.3896            0.9751                   50   \n",
      "3                 0.2697            1.0000                   74   \n",
      "4                 0.2372            1.0000                   65   \n",
      "\n",
      "   mega_whale_volume_eth  net_flow_ma7  non_exchange_ratio  \\\n",
      "0            108354.7154    -7203.9190              0.7101   \n",
      "1            147540.1401   -59329.7899              0.3673   \n",
      "2            196152.9223    53454.7638              0.6104   \n",
      "3            222378.6261    16368.8473              0.7303   \n",
      "4            334844.6762    -8432.1986              0.7628   \n",
      "\n",
      "   non_exchange_tx_count  ...  net_exchange_flow_ratio  \\\n",
      "0                     24  ...                -0.046363   \n",
      "1                     18  ...                -0.067329   \n",
      "2                     38  ...                 0.059721   \n",
      "3                     45  ...                -0.070792   \n",
      "4                     49  ...                -0.081152   \n",
      "\n",
      "   smart_contract_ratio_delta_1d  tx_per_active_delta_1d  \\\n",
      "0                            NaN                     NaN   \n",
      "1                            NaN                     NaN   \n",
      "2                            NaN                     NaN   \n",
      "3                      -0.001002                  0.0820   \n",
      "4                       0.014817                  0.0344   \n",
      "\n",
      "   tx_per_active_zscore_90d  whale_exchange_asymmetry  \\\n",
      "0                    0.0000                 -0.220737   \n",
      "1                    0.0000                 -0.145227   \n",
      "2                    0.0000                  0.274527   \n",
      "3                    0.7071                 -0.312723   \n",
      "4                    0.8405                 -0.351500   \n",
      "\n",
      "   whale_exchange_flow_ratio  whale_tx_zscore_90d  whale_volume_ratio  \\\n",
      "0                  -0.041071               0.0000            0.787476   \n",
      "1                  -0.064300               0.0000            0.828582   \n",
      "2                   0.065927               0.0000            0.802250   \n",
      "3                  -0.063788               0.0555            0.801580   \n",
      "4                  -0.077298               0.9225            0.836426   \n",
      "\n",
      "   whale_volume_ratio_delta_1d  whale_volume_ratio_delta_3d  \n",
      "0                          NaN                          NaN  \n",
      "1                          NaN                          NaN  \n",
      "2                          NaN                          NaN  \n",
      "3                    -0.000671                          NaN  \n",
      "4                     0.034846                          NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "ðŸ“‹ Columns:\n",
      "['block_date', 'deposit_tx_count', 'deposit_withdrawal_ratio', 'exchange_volume_ratio', 'mega_whale_ratio', 'mega_whale_tx_count', 'mega_whale_volume_eth', 'net_flow_ma7', 'non_exchange_ratio', 'non_exchange_tx_count', 'non_exchange_volume_eth', 'std_whale_tx_size_eth', 'whale_exchange_deposits_eth', 'whale_exchange_withdrawals_eth', 'whale_net_exchange_flow_eth', 'whale_tx_count', 'whale_volume_eth', 'withdrawal_tx_count', 'btc_price', 'eth_price', 'block_fullness_delta_1d', 'eth_burned_delta_1d', 'eth_burned_zscore_90d', 'exchange_flow_share', 'median_gas_delta_1d', 'median_gas_delta_7d', 'net_exchange_flow_ratio', 'smart_contract_ratio_delta_1d', 'tx_per_active_delta_1d', 'tx_per_active_zscore_90d', 'whale_exchange_asymmetry', 'whale_exchange_flow_ratio', 'whale_tx_zscore_90d', 'whale_volume_ratio', 'whale_volume_ratio_delta_1d', 'whale_volume_ratio_delta_3d']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"ðŸ“‚ Loading datasets...\")\n",
    "df_whales = pd.read_csv('whale_ml_ready.csv')\n",
    "df_market_intent = pd.read_csv('market_intent_ml_ready.csv')\n",
    "df_btc = pd.read_csv('data/price_cache/btc.csv')\n",
    "df_eth = pd.read_csv('data/price_cache/eth.csv')\n",
    "\n",
    "# Convert dates to datetime (UTC timezone-aware for consistency)\n",
    "print(\"ðŸ“… Converting dates...\")\n",
    "df_whales['block_date'] = pd.to_datetime(df_whales['block_date'], utc=True)\n",
    "df_market_intent['block_date'] = pd.to_datetime(df_market_intent['block_date'], utc=True)\n",
    "df_btc['date'] = pd.to_datetime(df_btc['date'], utc=True)\n",
    "df_eth['date'] = pd.to_datetime(df_eth['date'], utc=True)\n",
    "\n",
    "print(f\"âœ… Whales: {len(df_whales)} rows\")\n",
    "print(f\"âœ… Market Intent: {len(df_market_intent)} rows\")\n",
    "print(f\"âœ… BTC Prices: {len(df_btc)} rows\")\n",
    "print(f\"âœ… ETH Prices: {len(df_eth)} rows\")\n",
    "\n",
    "# Merge prices first\n",
    "print(\"\\nðŸ”— Merging price data...\")\n",
    "df_prices = pd.merge(\n",
    "    df_btc,\n",
    "    df_eth,\n",
    "    on='date',\n",
    "    how='outer',\n",
    "    suffixes=('_btc', '_eth')\n",
    ").sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Combined prices: {len(df_prices)} rows\")\n",
    "\n",
    "# Merge whale data with prices\n",
    "print(\"\\nðŸ”— Merging whale data with prices...\")\n",
    "df_merged = pd.merge(\n",
    "    df_whales,\n",
    "    df_prices,\n",
    "    left_on='block_date',\n",
    "    right_on='date',\n",
    "    how='left'\n",
    ").drop(columns=['date'])  # Remove duplicate date column\n",
    "\n",
    "print(f\"âœ… After whale + prices: {len(df_merged)} rows\")\n",
    "\n",
    "# Merge market intent data\n",
    "print(\"\\nðŸ”— Merging market intent data...\")\n",
    "df_final = pd.merge(\n",
    "    df_merged,\n",
    "    df_market_intent,\n",
    "    on='block_date',\n",
    "    how='left',\n",
    "    suffixes=('', '_intent')\n",
    ")\n",
    "\n",
    "print(f\"âœ… Final merged dataset: {len(df_final)} rows\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ” Checking data quality...\")\n",
    "missing_btc = df_final['btc_price'].isna().sum()\n",
    "missing_eth = df_final['eth_price'].isna().sum()\n",
    "missing_intent = df_final[[col for col in df_final.columns if 'intent' in col.lower()]].isna().sum().sum()\n",
    "\n",
    "print(f\"   Missing BTC prices: {missing_btc}\")\n",
    "print(f\"   Missing ETH prices: {missing_eth}\")\n",
    "print(f\"   Missing intent data: {missing_intent}\")\n",
    "\n",
    "# Display date range\n",
    "print(f\"\\nðŸ“… Date range: {df_final['block_date'].min().date()} â†’ {df_final['block_date'].max().date()}\")\n",
    "print(f\"ðŸ“Š Total columns: {len(df_final.columns)}\")\n",
    "\n",
    "# Save merged dataset\n",
    "OUTPUT_FILE = \"merged_ml_dataset.csv\"\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved: {OUTPUT_FILE}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nðŸ“‹ Sample of merged data:\")\n",
    "print(df_final.head())\n",
    "print(\"\\nðŸ“‹ Columns:\")\n",
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb010934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_date</th>\n",
       "      <th>deposit_tx_count</th>\n",
       "      <th>deposit_withdrawal_ratio</th>\n",
       "      <th>exchange_volume_ratio</th>\n",
       "      <th>mega_whale_ratio</th>\n",
       "      <th>mega_whale_tx_count</th>\n",
       "      <th>mega_whale_volume_eth</th>\n",
       "      <th>net_flow_ma7</th>\n",
       "      <th>non_exchange_ratio</th>\n",
       "      <th>non_exchange_tx_count</th>\n",
       "      <th>...</th>\n",
       "      <th>net_exchange_flow_ratio</th>\n",
       "      <th>smart_contract_ratio_delta_1d</th>\n",
       "      <th>tx_per_active_delta_1d</th>\n",
       "      <th>tx_per_active_zscore_90d</th>\n",
       "      <th>whale_exchange_asymmetry</th>\n",
       "      <th>whale_exchange_flow_ratio</th>\n",
       "      <th>whale_tx_zscore_90d</th>\n",
       "      <th>whale_volume_ratio</th>\n",
       "      <th>whale_volume_ratio_delta_1d</th>\n",
       "      <th>whale_volume_ratio_delta_3d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-24 00:00:00+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.5554</td>\n",
       "      <td>0.2899</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>36</td>\n",
       "      <td>1.083547e+05</td>\n",
       "      <td>-7203.9190</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.220737</td>\n",
       "      <td>-0.041071</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.787476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 00:00:00+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>4.4874</td>\n",
       "      <td>0.6327</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>36</td>\n",
       "      <td>1.475401e+05</td>\n",
       "      <td>-59329.7899</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.145227</td>\n",
       "      <td>-0.064300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.828582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.3896</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>50</td>\n",
       "      <td>1.961529e+05</td>\n",
       "      <td>53454.7638</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.274527</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.802250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-27 00:00:00+00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0556</td>\n",
       "      <td>0.2697</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>74</td>\n",
       "      <td>2.223786e+05</td>\n",
       "      <td>16368.8473</td>\n",
       "      <td>0.7303</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070792</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.7071</td>\n",
       "      <td>-0.312723</td>\n",
       "      <td>-0.063788</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.801580</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-28 00:00:00+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>6.4245</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>65</td>\n",
       "      <td>3.348447e+05</td>\n",
       "      <td>-8432.1986</td>\n",
       "      <td>0.7628</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081152</td>\n",
       "      <td>0.014817</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.8405</td>\n",
       "      <td>-0.351500</td>\n",
       "      <td>-0.077298</td>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.836426</td>\n",
       "      <td>0.034846</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2025-12-20 00:00:00+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0293</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>40</td>\n",
       "      <td>1.632716e+05</td>\n",
       "      <td>-91552.7172</td>\n",
       "      <td>0.7358</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051241</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>-1.2810</td>\n",
       "      <td>-0.238883</td>\n",
       "      <td>-0.046222</td>\n",
       "      <td>-1.8828</td>\n",
       "      <td>0.851792</td>\n",
       "      <td>-0.103314</td>\n",
       "      <td>-0.107734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2025-12-21 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>32</td>\n",
       "      <td>1.928101e+05</td>\n",
       "      <td>-100668.3469</td>\n",
       "      <td>0.4752</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055381</td>\n",
       "      <td>0.057881</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.9035</td>\n",
       "      <td>0.171106</td>\n",
       "      <td>0.059485</td>\n",
       "      <td>-2.4013</td>\n",
       "      <td>0.889958</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>-0.052551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2025-12-22 00:00:00+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>2.7336</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>70</td>\n",
       "      <td>6.105957e+05</td>\n",
       "      <td>-95628.5165</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066520</td>\n",
       "      <td>-0.066476</td>\n",
       "      <td>-0.4239</td>\n",
       "      <td>-1.5519</td>\n",
       "      <td>-0.216851</td>\n",
       "      <td>-0.064440</td>\n",
       "      <td>0.7183</td>\n",
       "      <td>0.936694</td>\n",
       "      <td>0.046736</td>\n",
       "      <td>-0.018412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2025-12-23 00:00:00+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>0.2043</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>69</td>\n",
       "      <td>1.014827e+06</td>\n",
       "      <td>-72486.9484</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029405</td>\n",
       "      <td>0.041258</td>\n",
       "      <td>-0.1504</td>\n",
       "      <td>-2.6310</td>\n",
       "      <td>-0.127150</td>\n",
       "      <td>-0.028212</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.104255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2025-12-24 00:00:00+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>3.6162</td>\n",
       "      <td>0.4103</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>57</td>\n",
       "      <td>6.367684e+05</td>\n",
       "      <td>-85556.8204</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126877</td>\n",
       "      <td>-0.038949</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>-1.7725</td>\n",
       "      <td>-0.398989</td>\n",
       "      <td>-0.124283</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>0.940054</td>\n",
       "      <td>-0.015993</td>\n",
       "      <td>0.050096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    block_date  deposit_tx_count  deposit_withdrawal_ratio  \\\n",
       "0    2022-12-24 00:00:00+00:00                11                    1.5554   \n",
       "1    2022-12-25 00:00:00+00:00                10                    4.4874   \n",
       "2    2022-12-26 00:00:00+00:00                 8                    0.1890   \n",
       "3    2022-12-27 00:00:00+00:00                19                    2.0556   \n",
       "4    2022-12-28 00:00:00+00:00                14                    6.4245   \n",
       "...                        ...               ...                       ...   \n",
       "1092 2025-12-20 00:00:00+00:00                 9                   11.0293   \n",
       "1093 2025-12-21 00:00:00+00:00                 8                    0.8793   \n",
       "1094 2025-12-22 00:00:00+00:00                14                    2.7336   \n",
       "1095 2025-12-23 00:00:00+00:00                11                    1.6121   \n",
       "1096 2025-12-24 00:00:00+00:00                13                    3.6162   \n",
       "\n",
       "      exchange_volume_ratio  mega_whale_ratio  mega_whale_tx_count  \\\n",
       "0                    0.2899            0.9475                   36   \n",
       "1                    0.6327            1.0000                   36   \n",
       "2                    0.3896            0.9751                   50   \n",
       "3                    0.2697            1.0000                   74   \n",
       "4                    0.2372            1.0000                   65   \n",
       "...                     ...               ...                  ...   \n",
       "1092                 0.2642            1.0000                   40   \n",
       "1093                 0.5248            1.0000                   32   \n",
       "1094                 0.3557            1.0000                   70   \n",
       "1095                 0.2043            1.0000                   69   \n",
       "1096                 0.4103            1.0000                   57   \n",
       "\n",
       "      mega_whale_volume_eth  net_flow_ma7  non_exchange_ratio  \\\n",
       "0              1.083547e+05    -7203.9190              0.7101   \n",
       "1              1.475401e+05   -59329.7899              0.3673   \n",
       "2              1.961529e+05    53454.7638              0.6104   \n",
       "3              2.223786e+05    16368.8473              0.7303   \n",
       "4              3.348447e+05    -8432.1986              0.7628   \n",
       "...                     ...           ...                 ...   \n",
       "1092           1.632716e+05   -91552.7172              0.7358   \n",
       "1093           1.928101e+05  -100668.3469              0.4752   \n",
       "1094           6.105957e+05   -95628.5165              0.6443   \n",
       "1095           1.014827e+06   -72486.9484              0.7957   \n",
       "1096           6.367684e+05   -85556.8204              0.5897   \n",
       "\n",
       "      non_exchange_tx_count  ...  net_exchange_flow_ratio  \\\n",
       "0                        24  ...                -0.046363   \n",
       "1                        18  ...                -0.067329   \n",
       "2                        38  ...                 0.059721   \n",
       "3                        45  ...                -0.070792   \n",
       "4                        49  ...                -0.081152   \n",
       "...                     ...  ...                      ...   \n",
       "1092                     29  ...                -0.051241   \n",
       "1093                     19  ...                 0.055381   \n",
       "1094                     51  ...                -0.066520   \n",
       "1095                     53  ...                -0.029405   \n",
       "1096                     40  ...                -0.126877   \n",
       "\n",
       "      smart_contract_ratio_delta_1d  tx_per_active_delta_1d  \\\n",
       "0                               NaN                     NaN   \n",
       "1                               NaN                     NaN   \n",
       "2                               NaN                     NaN   \n",
       "3                         -0.001002                  0.0820   \n",
       "4                          0.014817                  0.0344   \n",
       "...                             ...                     ...   \n",
       "1092                       0.013453                 -0.1327   \n",
       "1093                       0.057881                  0.3857   \n",
       "1094                      -0.066476                 -0.4239   \n",
       "1095                       0.041258                 -0.1504   \n",
       "1096                      -0.038949                  0.1057   \n",
       "\n",
       "      tx_per_active_zscore_90d  whale_exchange_asymmetry  \\\n",
       "0                       0.0000                 -0.220737   \n",
       "1                       0.0000                 -0.145227   \n",
       "2                       0.0000                  0.274527   \n",
       "3                       0.7071                 -0.312723   \n",
       "4                       0.8405                 -0.351500   \n",
       "...                        ...                       ...   \n",
       "1092                   -1.2810                 -0.238883   \n",
       "1093                    1.9035                  0.171106   \n",
       "1094                   -1.5519                 -0.216851   \n",
       "1095                   -2.6310                 -0.127150   \n",
       "1096                   -1.7725                 -0.398989   \n",
       "\n",
       "      whale_exchange_flow_ratio  whale_tx_zscore_90d  whale_volume_ratio  \\\n",
       "0                     -0.041071               0.0000            0.787476   \n",
       "1                     -0.064300               0.0000            0.828582   \n",
       "2                      0.065927               0.0000            0.802250   \n",
       "3                     -0.063788               0.0555            0.801580   \n",
       "4                     -0.077298               0.9225            0.836426   \n",
       "...                         ...                  ...                 ...   \n",
       "1092                  -0.046222              -1.8828            0.851792   \n",
       "1093                   0.059485              -2.4013            0.889958   \n",
       "1094                  -0.064440               0.7183            0.936694   \n",
       "1095                  -0.028212               0.4125            0.956047   \n",
       "1096                  -0.124283               0.4733            0.940054   \n",
       "\n",
       "      whale_volume_ratio_delta_1d  whale_volume_ratio_delta_3d  \n",
       "0                             NaN                          NaN  \n",
       "1                             NaN                          NaN  \n",
       "2                             NaN                          NaN  \n",
       "3                       -0.000671                          NaN  \n",
       "4                        0.034846                          NaN  \n",
       "...                           ...                          ...  \n",
       "1092                    -0.103314                    -0.107734  \n",
       "1093                     0.038166                    -0.052551  \n",
       "1094                     0.046736                    -0.018412  \n",
       "1095                     0.019353                     0.104255  \n",
       "1096                    -0.015993                     0.050096  \n",
       "\n",
       "[1097 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64dac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['block_date', 'deposit_tx_count', 'deposit_withdrawal_ratio',\n",
       "       'exchange_volume_ratio', 'mega_whale_ratio', 'mega_whale_tx_count',\n",
       "       'mega_whale_volume_eth', 'net_flow_ma7', 'non_exchange_ratio',\n",
       "       'non_exchange_tx_count', 'non_exchange_volume_eth',\n",
       "       'std_whale_tx_size_eth', 'whale_exchange_deposits_eth',\n",
       "       'whale_exchange_withdrawals_eth', 'whale_net_exchange_flow_eth',\n",
       "       'whale_tx_count', 'whale_volume_eth', 'withdrawal_tx_count',\n",
       "       'btc_price', 'eth_price', 'block_fullness_delta_1d',\n",
       "       'eth_burned_delta_1d', 'eth_burned_zscore_90d', 'exchange_flow_share',\n",
       "       'median_gas_delta_1d', 'median_gas_delta_7d', 'net_exchange_flow_ratio',\n",
       "       'smart_contract_ratio_delta_1d', 'tx_per_active_delta_1d',\n",
       "       'tx_per_active_zscore_90d', 'whale_exchange_asymmetry',\n",
       "       'whale_exchange_flow_ratio', 'whale_tx_zscore_90d',\n",
       "       'whale_volume_ratio', 'whale_volume_ratio_delta_1d',\n",
       "       'whale_volume_ratio_delta_3d'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0b93a",
   "metadata": {},
   "source": [
    "- Feature Engineering from version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77fb6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™ï¸ Engineering features...\n",
      "âœ… Features created: 42 total columns\n"
     ]
    }
   ],
   "source": [
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    # Returns\n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    \n",
    "    # Volatility\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target: next day price direction\"\"\"\n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "# Feature engineering pipeline\n",
    "print(\"\\nâš™ï¸ Engineering features...\")\n",
    "df_merged = add_price_features(df_merged, 'eth_price', 'eth')\n",
    "df_merged = add_price_features(df_merged, 'btc_price', 'btc')\n",
    "df_merged = add_correlation_features(df_merged)\n",
    "df_merged = create_target(df_merged)\n",
    "print(f\"âœ… Features created: {len(df_merged.columns)} total columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ee6ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['block_date', 'deposit_tx_count', 'deposit_withdrawal_ratio',\n",
       "       'exchange_volume_ratio', 'mega_whale_ratio', 'mega_whale_tx_count',\n",
       "       'mega_whale_volume_eth', 'net_flow_ma7', 'non_exchange_ratio',\n",
       "       'non_exchange_tx_count', 'non_exchange_volume_eth',\n",
       "       'std_whale_tx_size_eth', 'whale_exchange_deposits_eth',\n",
       "       'whale_exchange_withdrawals_eth', 'whale_net_exchange_flow_eth',\n",
       "       'whale_tx_count', 'whale_volume_eth', 'withdrawal_tx_count',\n",
       "       'btc_price', 'eth_price', 'eth_daily_return', 'eth_log_return',\n",
       "       'eth_vol7', 'eth_vol30', 'eth_rsi', 'eth_ret_lag1', 'eth_ret_lag3',\n",
       "       'eth_ret_lag7', 'btc_daily_return', 'btc_log_return', 'btc_vol7',\n",
       "       'btc_vol30', 'btc_rsi', 'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
       "       'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d',\n",
       "       'eth_outperformance', 'next_day_return', 'next_day_price_direction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ce9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading merged dataset...\n",
      "âœ… Loaded 1097 rows, 36 columns\n",
      "\n",
      "âš™ï¸ Engineering features...\n",
      "âœ… Features created: 58 columns\n",
      "Date range: 2022-12-24 â†’ 2025-12-24\n",
      "\n",
      "======================================================================\n",
      "MODEL A: A Price Only\n",
      "======================================================================\n",
      "Features: 20\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1077\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.2%\n",
      "  Class 1 (up):   51.8%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5564\n",
      "  Precision: 0.5525\n",
      "  Recall:    0.8343\n",
      "  F1 Score:  0.6620\n",
      "  ROC AUC:   0.5721\n",
      "\n",
      "âœ… Model A (Price-only baseline) complete!\n",
      "\n",
      "Next steps:\n",
      "  1. Add whale features for Model B\n",
      "  2. Add market intent for Model C\n",
      "  3. Compare all models to see what adds value\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: ABLATION TESTING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "class AblationTester:\n",
    "    \"\"\"Test feature group contributions via ablation study\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='next_day_price_direction'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.results = {}\n",
    "        \n",
    "    def define_feature_groups(self):\n",
    "        \"\"\"Define Model A-C feature sets\"\"\"\n",
    "        \n",
    "        # Model A: Price-only baseline\n",
    "        self.groups = {\n",
    "            'A_price_only': [\n",
    "                'eth_daily_return', 'eth_log_return', 'eth_vol7', 'eth_vol30',\n",
    "                'eth_rsi', 'eth_ret_lag1', 'eth_ret_lag3', 'eth_ret_lag7',\n",
    "                'btc_daily_return', 'btc_log_return', 'btc_vol7', 'btc_vol30',\n",
    "                'btc_rsi', 'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
    "                'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d',\n",
    "                'eth_outperformance'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Verify features exist\n",
    "        missing = [f for f in self.groups['A_price_only'] if f not in self.df.columns]\n",
    "        if missing:\n",
    "            print(f\"âš ï¸ Missing features: {missing}\")\n",
    "            self.groups['A_price_only'] = [f for f in self.groups['A_price_only'] \n",
    "                                            if f in self.df.columns]\n",
    "        \n",
    "        return self.groups\n",
    "    \n",
    "    def prepare_data(self, features):\n",
    "        \"\"\"Clean data for modeling\"\"\"\n",
    "        df_clean = self.df[features + [self.target_col, 'block_date']].copy()\n",
    "        \n",
    "        # Remove rows with missing target or features\n",
    "        df_clean = df_clean.dropna(subset=[self.target_col])\n",
    "        df_clean = df_clean.dropna(subset=features)\n",
    "        \n",
    "        # Sort by date\n",
    "        df_clean = df_clean.sort_values('block_date').reset_index(drop=True)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def time_series_cv(self, X, y, n_splits=5):\n",
    "        \"\"\"Walk-forward time series cross-validation\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        scores = {\n",
    "            'accuracy': [], 'precision': [], 'recall': [], \n",
    "            'f1': [], 'roc_auc': []\n",
    "        }\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "            scores['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "            scores['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "            scores['roc_auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "    \n",
    "    def run_ablation(self, model_name='A_price_only', n_splits=5):\n",
    "        \"\"\"Execute ablation test for a feature group\"\"\"\n",
    "        \n",
    "        features = self.groups[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MODEL {model_name.split('_')[0]}: {model_name.replace('_', ' ').title()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Features: {len(features)}\")\n",
    "        print(f\"Feature list: {', '.join(features[:5])}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = self.prepare_data(features)\n",
    "        print(f\"Clean samples: {len(df_clean)}\")\n",
    "        print(f\"Date range: {df_clean['block_date'].min().date()} â†’ \"\n",
    "              f\"{df_clean['block_date'].max().date()}\")\n",
    "        \n",
    "        X = df_clean[features]\n",
    "        y = df_clean[self.target_col]\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = y.value_counts(normalize=True)\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        print(f\"  Class 0 (down): {class_dist[0]:.1%}\")\n",
    "        print(f\"  Class 1 (up):   {class_dist[1]:.1%}\")\n",
    "        \n",
    "        # Run CV\n",
    "        print(f\"\\nRunning {n_splits}-fold walk-forward CV...\")\n",
    "        scores = self.time_series_cv(X, y, n_splits)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'features': features,\n",
    "            'n_features': len(features),\n",
    "            'n_samples': len(df_clean),\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nðŸ“Š RESULTS:\")\n",
    "        print(f\"  Accuracy:  {scores['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {scores['f1']:.4f}\")\n",
    "        print(f\"  ROC AUC:   {scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all tested models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models tested yet!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ABLATION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            name: res['scores'] for name, res in self.results.items()\n",
    "        }).T\n",
    "        \n",
    "        comparison['n_features'] = [res['n_features'] for res in self.results.values()]\n",
    "        comparison = comparison[['n_features', 'accuracy', 'precision', \n",
    "                                 'recall', 'f1', 'roc_auc']]\n",
    "        \n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = comparison['accuracy'].idxmax()\n",
    "        best_f1 = comparison['f1'].idxmax()\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Accuracy: {best_acc} ({comparison.loc[best_acc, 'accuracy']:.4f})\")\n",
    "        print(f\"ðŸ† Best F1 Score: {best_f1} ({comparison.loc[best_f1, 'f1']:.4f})\")\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target: next day price direction\"\"\"\n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load merged dataset\n",
    "    print(\"ðŸ“‚ Loading merged dataset...\")\n",
    "    df_merged = pd.read_csv('merged_ml_dataset.csv')\n",
    "    df_merged['block_date'] = pd.to_datetime(df_merged['block_date'], utc=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df_merged)} rows, {len(df_merged.columns)} columns\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\nâš™ï¸ Engineering features...\")\n",
    "    df_merged = add_price_features(df_merged, 'eth_price', 'eth')\n",
    "    df_merged = add_price_features(df_merged, 'btc_price', 'btc')\n",
    "    df_merged = add_correlation_features(df_merged)\n",
    "    df_merged = create_target(df_merged)\n",
    "    \n",
    "    print(f\"âœ… Features created: {len(df_merged.columns)} columns\")\n",
    "    print(f\"Date range: {df_merged['block_date'].min().date()} â†’ \"\n",
    "          f\"{df_merged['block_date'].max().date()}\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AblationTester(df_merged)\n",
    "    \n",
    "    # Define feature groups\n",
    "    tester.define_feature_groups()\n",
    "    \n",
    "    # Run Model A: Price-only baseline\n",
    "    tester.run_ablation('A_price_only', n_splits=5)\n",
    "    \n",
    "    print(\"\\nâœ… Model A (Price-only baseline) complete!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Add whale features for Model B\")\n",
    "    print(\"  2. Add market intent for Model C\")\n",
    "    print(\"  3. Compare all models to see what adds value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26db92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading merged dataset...\n",
      "âœ… Loaded 1097 rows, 36 columns\n",
      "\n",
      "âš™ï¸ Engineering features...\n",
      "âœ… Features created: 58 columns\n",
      "Date range: 2022-12-24 â†’ 2025-12-24\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: BASELINE TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL A: A Price Only\n",
      "======================================================================\n",
      "Features: 20\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1077\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.2%\n",
      "  Class 1 (up):   51.8%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5564\n",
      "  Precision: 0.5525\n",
      "  Recall:    0.8343\n",
      "  F1 Score:  0.6620\n",
      "  ROC AUC:   0.5721\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: ON-CHAIN SIGNAL TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL B: B Onchain Only\n",
      "======================================================================\n",
      "Features: 27\n",
      "Feature list: deposit_tx_count, withdrawal_tx_count, deposit_withdrawal_ratio, exchange_volume_ratio, exchange_flow_share...\n",
      "Clean samples: 1086\n",
      "Date range: 2023-01-02 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 47.9%\n",
      "  Class 1 (up):   52.1%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5193\n",
      "  Precision: 0.5588\n",
      "  Recall:    0.4697\n",
      "  F1 Score:  0.4879\n",
      "  ROC AUC:   0.5389\n",
      "\n",
      "======================================================================\n",
      "ABLATION COMPARISON\n",
      "======================================================================\n",
      "                n_features  accuracy  precision    recall        f1   roc_auc\n",
      "A_price_only            20  0.556425   0.552493  0.834288  0.661993  0.572111\n",
      "B_onchain_only          27  0.519337   0.558822  0.469713  0.487912  0.538859\n",
      "\n",
      "ðŸ† Best Accuracy: A_price_only (0.5564)\n",
      "ðŸ† Best F1 Score: A_price_only (0.6620)\n",
      "\n",
      "âœ… Ablation Phase 1-2 complete!\n",
      "\n",
      "ðŸ” Key Insights:\n",
      "  â€¢ Model A (price): 55.64% accuracy\n",
      "  â€¢ Model B (on-chain): 51.93% accuracy\n",
      "  â€¢ On-chain signals underperform by 3.71%\n",
      "\n",
      "Next steps:\n",
      "  â€¢ If Model B > 50.5%: on-chain has signal\n",
      "  â€¢ Build Model C (combined) to test complementarity\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: ABLATION TESTING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "class AblationTester:\n",
    "    \"\"\"Test feature group contributions via ablation study\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='next_day_price_direction'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.results = {}\n",
    "        \n",
    "    def define_feature_groups(self):\n",
    "        \"\"\"Define Model A-C feature sets\"\"\"\n",
    "        \n",
    "        # Model A: Price-only baseline\n",
    "        self.groups = {\n",
    "            'A_price_only': [\n",
    "                'eth_daily_return', 'eth_log_return', 'eth_vol7', 'eth_vol30',\n",
    "                'eth_rsi', 'eth_ret_lag1', 'eth_ret_lag3', 'eth_ret_lag7',\n",
    "                'btc_daily_return', 'btc_log_return', 'btc_vol7', 'btc_vol30',\n",
    "                'btc_rsi', 'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
    "                'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d',\n",
    "                'eth_outperformance'\n",
    "            ],\n",
    "            \n",
    "            # Model B: On-chain only\n",
    "            'B_onchain_only': [\n",
    "                'deposit_tx_count', 'withdrawal_tx_count', 'deposit_withdrawal_ratio',\n",
    "                'exchange_volume_ratio', 'exchange_flow_share', 'net_exchange_flow_ratio',\n",
    "                'whale_exchange_deposits_eth', 'whale_exchange_withdrawals_eth',\n",
    "                'whale_net_exchange_flow_eth', 'whale_exchange_flow_ratio',\n",
    "                'whale_exchange_asymmetry', 'whale_tx_count', 'whale_volume_eth',\n",
    "                'whale_volume_ratio', 'whale_volume_ratio_delta_1d',\n",
    "                'whale_volume_ratio_delta_3d', 'whale_tx_zscore_90d',\n",
    "                'mega_whale_ratio', 'net_flow_ma7', 'tx_per_active_delta_1d',\n",
    "                'tx_per_active_zscore_90d', 'block_fullness_delta_1d',\n",
    "                'eth_burned_delta_1d', 'eth_burned_zscore_90d', 'median_gas_delta_1d',\n",
    "                'median_gas_delta_7d', 'smart_contract_ratio_delta_1d'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Verify features exist for all groups\n",
    "        for group_name, features in self.groups.items():\n",
    "            missing = [f for f in features if f not in self.df.columns]\n",
    "            if missing:\n",
    "                print(f\"âš ï¸ {group_name} missing: {missing}\")\n",
    "                self.groups[group_name] = [f for f in features if f in self.df.columns]\n",
    "        \n",
    "        return self.groups\n",
    "    \n",
    "    def prepare_data(self, features):\n",
    "        \"\"\"Clean data for modeling\"\"\"\n",
    "        df_clean = self.df[features + [self.target_col, 'block_date']].copy()\n",
    "        \n",
    "        # Remove rows with missing target or features\n",
    "        df_clean = df_clean.dropna(subset=[self.target_col])\n",
    "        df_clean = df_clean.dropna(subset=features)\n",
    "        \n",
    "        # Sort by date\n",
    "        df_clean = df_clean.sort_values('block_date').reset_index(drop=True)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def time_series_cv(self, X, y, n_splits=5):\n",
    "        \"\"\"Walk-forward time series cross-validation\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        scores = {\n",
    "            'accuracy': [], 'precision': [], 'recall': [], \n",
    "            'f1': [], 'roc_auc': []\n",
    "        }\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "            scores['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "            scores['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "            scores['roc_auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "    \n",
    "    def run_ablation(self, model_name='A_price_only', n_splits=5):\n",
    "        \"\"\"Execute ablation test for a feature group\"\"\"\n",
    "        \n",
    "        features = self.groups[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MODEL {model_name.split('_')[0]}: {model_name.replace('_', ' ').title()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Features: {len(features)}\")\n",
    "        print(f\"Feature list: {', '.join(features[:5])}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = self.prepare_data(features)\n",
    "        print(f\"Clean samples: {len(df_clean)}\")\n",
    "        print(f\"Date range: {df_clean['block_date'].min().date()} â†’ \"\n",
    "              f\"{df_clean['block_date'].max().date()}\")\n",
    "        \n",
    "        X = df_clean[features]\n",
    "        y = df_clean[self.target_col]\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = y.value_counts(normalize=True)\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        print(f\"  Class 0 (down): {class_dist[0]:.1%}\")\n",
    "        print(f\"  Class 1 (up):   {class_dist[1]:.1%}\")\n",
    "        \n",
    "        # Run CV\n",
    "        print(f\"\\nRunning {n_splits}-fold walk-forward CV...\")\n",
    "        scores = self.time_series_cv(X, y, n_splits)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'features': features,\n",
    "            'n_features': len(features),\n",
    "            'n_samples': len(df_clean),\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nðŸ“Š RESULTS:\")\n",
    "        print(f\"  Accuracy:  {scores['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {scores['f1']:.4f}\")\n",
    "        print(f\"  ROC AUC:   {scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all tested models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models tested yet!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ABLATION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            name: res['scores'] for name, res in self.results.items()\n",
    "        }).T\n",
    "        \n",
    "        comparison['n_features'] = [res['n_features'] for res in self.results.values()]\n",
    "        comparison = comparison[['n_features', 'accuracy', 'precision', \n",
    "                                 'recall', 'f1', 'roc_auc']]\n",
    "        \n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = comparison['accuracy'].idxmax()\n",
    "        best_f1 = comparison['f1'].idxmax()\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Accuracy: {best_acc} ({comparison.loc[best_acc, 'accuracy']:.4f})\")\n",
    "        print(f\"ðŸ† Best F1 Score: {best_f1} ({comparison.loc[best_f1, 'f1']:.4f})\")\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target: next day price direction\"\"\"\n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load merged dataset\n",
    "    print(\"ðŸ“‚ Loading merged dataset...\")\n",
    "    df_merged = pd.read_csv('merged_ml_dataset.csv')\n",
    "    df_merged['block_date'] = pd.to_datetime(df_merged['block_date'], utc=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df_merged)} rows, {len(df_merged.columns)} columns\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\nâš™ï¸ Engineering features...\")\n",
    "    df_merged = add_price_features(df_merged, 'eth_price', 'eth')\n",
    "    df_merged = add_price_features(df_merged, 'btc_price', 'btc')\n",
    "    df_merged = add_correlation_features(df_merged)\n",
    "    df_merged = create_target(df_merged)\n",
    "    \n",
    "    print(f\"âœ… Features created: {len(df_merged.columns)} columns\")\n",
    "    print(f\"Date range: {df_merged['block_date'].min().date()} â†’ \"\n",
    "          f\"{df_merged['block_date'].max().date()}\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AblationTester(df_merged)\n",
    "    \n",
    "    # Define feature groups\n",
    "    tester.define_feature_groups()\n",
    "    \n",
    "    # Run Model A: Price-only baseline\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: BASELINE TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('A_price_only', n_splits=5)\n",
    "    \n",
    "    # Run Model B: On-chain only\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: ON-CHAIN SIGNAL TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('B_onchain_only', n_splits=5)\n",
    "    \n",
    "    # Compare results\n",
    "    comparison = tester.compare_models()\n",
    "    \n",
    "    print(\"\\nâœ… Ablation Phase 1-2 complete!\")\n",
    "    print(\"\\nðŸ” Key Insights:\")\n",
    "    print(f\"  â€¢ Model A (price): {tester.results['A_price_only']['scores']['accuracy']:.2%} accuracy\")\n",
    "    print(f\"  â€¢ Model B (on-chain): {tester.results['B_onchain_only']['scores']['accuracy']:.2%} accuracy\")\n",
    "    \n",
    "    acc_diff = tester.results['B_onchain_only']['scores']['accuracy'] - tester.results['A_price_only']['scores']['accuracy']\n",
    "    if acc_diff > 0:\n",
    "        print(f\"  â€¢ On-chain signals add {acc_diff:.2%} raw accuracy\")\n",
    "    else:\n",
    "        print(f\"  â€¢ On-chain signals underperform by {abs(acc_diff):.2%}\")\n",
    "    \n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  â€¢ If Model B > 50.5%: on-chain has signal\")\n",
    "    print(\"  â€¢ Build Model C (combined) to test complementarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7dae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading merged dataset...\n",
      "âœ… Loaded 1097 rows, 36 columns\n",
      "\n",
      "âš™ï¸ Engineering features...\n",
      "âœ… Features created: 58 columns\n",
      "Date range: 2022-12-24 â†’ 2025-12-24\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: BASELINE TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL A: A Price Only\n",
      "======================================================================\n",
      "Features: 20\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1077\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.2%\n",
      "  Class 1 (up):   51.8%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5564\n",
      "  Precision: 0.5525\n",
      "  Recall:    0.8343\n",
      "  F1 Score:  0.6620\n",
      "  ROC AUC:   0.5721\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: ON-CHAIN SIGNAL TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL B: B Onchain Only\n",
      "======================================================================\n",
      "Features: 27\n",
      "Feature list: deposit_tx_count, withdrawal_tx_count, deposit_withdrawal_ratio, exchange_volume_ratio, exchange_flow_share...\n",
      "Clean samples: 1086\n",
      "Date range: 2023-01-02 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 47.9%\n",
      "  Class 1 (up):   52.1%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5193\n",
      "  Precision: 0.5588\n",
      "  Recall:    0.4697\n",
      "  F1 Score:  0.4879\n",
      "  ROC AUC:   0.5389\n",
      "\n",
      "======================================================================\n",
      "PHASE 3: HYBRID MODEL (PRICE + ON-CHAIN)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL C: C Hybrid\n",
      "======================================================================\n",
      "Features: 47\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1075\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.3%\n",
      "  Class 1 (up):   51.7%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5397\n",
      "  Precision: 0.5736\n",
      "  Recall:    0.6838\n",
      "  F1 Score:  0.5839\n",
      "  ROC AUC:   0.5707\n",
      "\n",
      "======================================================================\n",
      "ABLATION COMPARISON\n",
      "======================================================================\n",
      "                n_features  accuracy  precision    recall        f1   roc_auc\n",
      "A_price_only            20  0.556425   0.552493  0.834288  0.661993  0.572111\n",
      "B_onchain_only          27  0.519337   0.558822  0.469713  0.487912  0.538859\n",
      "C_hybrid                47  0.539665   0.573636  0.683770  0.583876  0.570721\n",
      "\n",
      "ðŸ† Best Accuracy: A_price_only (0.5564)\n",
      "ðŸ† Best F1 Score: A_price_only (0.6620)\n",
      "\n",
      "======================================================================\n",
      "FINAL ABLATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Accuracy Comparison:\n",
      "  Model A (price):    55.64%\n",
      "  Model B (on-chain): 51.93% (1.93% above random)\n",
      "  Model C (hybrid):   53.97%\n",
      "\n",
      "ðŸŽ¯ Incremental Value:\n",
      "  On-chain adds: -1.68% accuracy\n",
      "  âŒ FAIL: On-chain adds no value (or hurts)\n",
      "\n",
      "ðŸ“ˆ ROC AUC:\n",
      "  Model C: 0.5707 vs Model A: 0.5721 (-0.0014)\n",
      "\n",
      "ðŸ’¡ Next Steps:\n",
      "  â€¢ On-chain signals may be regime-specific\n",
      "  â€¢ Try interaction features (price_vol * whale_flow)\n",
      "  â€¢ Consider threshold-based rules instead of ML\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: ABLATION TESTING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "class AblationTester:\n",
    "    \"\"\"Test feature group contributions via ablation study\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='next_day_price_direction'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.results = {}\n",
    "        \n",
    "    def define_feature_groups(self):\n",
    "        \"\"\"Define Model A-C feature sets\"\"\"\n",
    "        \n",
    "        # Model A: Price-only baseline\n",
    "        self.groups = {\n",
    "            'A_price_only': [\n",
    "                'eth_daily_return', 'eth_log_return', 'eth_vol7', 'eth_vol30',\n",
    "                'eth_rsi', 'eth_ret_lag1', 'eth_ret_lag3', 'eth_ret_lag7',\n",
    "                'btc_daily_return', 'btc_log_return', 'btc_vol7', 'btc_vol30',\n",
    "                'btc_rsi', 'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
    "                'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d',\n",
    "                'eth_outperformance'\n",
    "            ],\n",
    "            \n",
    "            # Model B: On-chain only\n",
    "            'B_onchain_only': [\n",
    "                'deposit_tx_count', 'withdrawal_tx_count', 'deposit_withdrawal_ratio',\n",
    "                'exchange_volume_ratio', 'exchange_flow_share', 'net_exchange_flow_ratio',\n",
    "                'whale_exchange_deposits_eth', 'whale_exchange_withdrawals_eth',\n",
    "                'whale_net_exchange_flow_eth', 'whale_exchange_flow_ratio',\n",
    "                'whale_exchange_asymmetry', 'whale_tx_count', 'whale_volume_eth',\n",
    "                'whale_volume_ratio', 'whale_volume_ratio_delta_1d',\n",
    "                'whale_volume_ratio_delta_3d', 'whale_tx_zscore_90d',\n",
    "                'mega_whale_ratio', 'net_flow_ma7', 'tx_per_active_delta_1d',\n",
    "                'tx_per_active_zscore_90d', 'block_fullness_delta_1d',\n",
    "                'eth_burned_delta_1d', 'eth_burned_zscore_90d', 'median_gas_delta_1d',\n",
    "                'median_gas_delta_7d', 'smart_contract_ratio_delta_1d'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Model C: Hybrid (A + B, no raw prices)\n",
    "        self.groups['C_hybrid'] = (\n",
    "            self.groups['A_price_only'] + \n",
    "            self.groups['B_onchain_only']\n",
    "        )\n",
    "        \n",
    "        # Verify features exist for all groups\n",
    "        for group_name, features in self.groups.items():\n",
    "            missing = [f for f in features if f not in self.df.columns]\n",
    "            if missing:\n",
    "                print(f\"âš ï¸ {group_name} missing: {missing}\")\n",
    "                self.groups[group_name] = [f for f in features if f in self.df.columns]\n",
    "        \n",
    "        return self.groups\n",
    "    \n",
    "    def prepare_data(self, features):\n",
    "        \"\"\"Clean data for modeling\"\"\"\n",
    "        df_clean = self.df[features + [self.target_col, 'block_date']].copy()\n",
    "        \n",
    "        # Remove rows with missing target or features\n",
    "        df_clean = df_clean.dropna(subset=[self.target_col])\n",
    "        df_clean = df_clean.dropna(subset=features)\n",
    "        \n",
    "        # Sort by date\n",
    "        df_clean = df_clean.sort_values('block_date').reset_index(drop=True)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def time_series_cv(self, X, y, n_splits=5):\n",
    "        \"\"\"Walk-forward time series cross-validation\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        scores = {\n",
    "            'accuracy': [], 'precision': [], 'recall': [], \n",
    "            'f1': [], 'roc_auc': []\n",
    "        }\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "            scores['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "            scores['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "            scores['roc_auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "    \n",
    "    def run_ablation(self, model_name='A_price_only', n_splits=5):\n",
    "        \"\"\"Execute ablation test for a feature group\"\"\"\n",
    "        \n",
    "        features = self.groups[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MODEL {model_name.split('_')[0]}: {model_name.replace('_', ' ').title()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Features: {len(features)}\")\n",
    "        print(f\"Feature list: {', '.join(features[:5])}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = self.prepare_data(features)\n",
    "        print(f\"Clean samples: {len(df_clean)}\")\n",
    "        print(f\"Date range: {df_clean['block_date'].min().date()} â†’ \"\n",
    "              f\"{df_clean['block_date'].max().date()}\")\n",
    "        \n",
    "        X = df_clean[features]\n",
    "        y = df_clean[self.target_col]\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = y.value_counts(normalize=True)\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        print(f\"  Class 0 (down): {class_dist[0]:.1%}\")\n",
    "        print(f\"  Class 1 (up):   {class_dist[1]:.1%}\")\n",
    "        \n",
    "        # Run CV\n",
    "        print(f\"\\nRunning {n_splits}-fold walk-forward CV...\")\n",
    "        scores = self.time_series_cv(X, y, n_splits)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'features': features,\n",
    "            'n_features': len(features),\n",
    "            'n_samples': len(df_clean),\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nðŸ“Š RESULTS:\")\n",
    "        print(f\"  Accuracy:  {scores['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {scores['f1']:.4f}\")\n",
    "        print(f\"  ROC AUC:   {scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all tested models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models tested yet!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ABLATION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            name: res['scores'] for name, res in self.results.items()\n",
    "        }).T\n",
    "        \n",
    "        comparison['n_features'] = [res['n_features'] for res in self.results.values()]\n",
    "        comparison = comparison[['n_features', 'accuracy', 'precision', \n",
    "                                 'recall', 'f1', 'roc_auc']]\n",
    "        \n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = comparison['accuracy'].idxmax()\n",
    "        best_f1 = comparison['f1'].idxmax()\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Accuracy: {best_acc} ({comparison.loc[best_acc, 'accuracy']:.4f})\")\n",
    "        print(f\"ðŸ† Best F1 Score: {best_f1} ({comparison.loc[best_f1, 'f1']:.4f})\")\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target: next day price direction\"\"\"\n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load merged dataset\n",
    "    print(\"ðŸ“‚ Loading merged dataset...\")\n",
    "    df_merged = pd.read_csv('merged_ml_dataset.csv')\n",
    "    df_merged['block_date'] = pd.to_datetime(df_merged['block_date'], utc=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df_merged)} rows, {len(df_merged.columns)} columns\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\nâš™ï¸ Engineering features...\")\n",
    "    df_merged = add_price_features(df_merged, 'eth_price', 'eth')\n",
    "    df_merged = add_price_features(df_merged, 'btc_price', 'btc')\n",
    "    df_merged = add_correlation_features(df_merged)\n",
    "    df_merged = create_target(df_merged)\n",
    "    \n",
    "    print(f\"âœ… Features created: {len(df_merged.columns)} columns\")\n",
    "    print(f\"Date range: {df_merged['block_date'].min().date()} â†’ \"\n",
    "          f\"{df_merged['block_date'].max().date()}\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AblationTester(df_merged)\n",
    "    \n",
    "    # Define feature groups\n",
    "    tester.define_feature_groups()\n",
    "    \n",
    "    # Run Model A: Price-only baseline\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: BASELINE TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('A_price_only', n_splits=5)\n",
    "    \n",
    "    # Run Model B: On-chain only\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: ON-CHAIN SIGNAL TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('B_onchain_only', n_splits=5)\n",
    "    \n",
    "    # Run Model C: Hybrid\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 3: HYBRID MODEL (PRICE + ON-CHAIN)\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('C_hybrid', n_splits=5)\n",
    "    \n",
    "    # Compare results\n",
    "    comparison = tester.compare_models()\n",
    "    \n",
    "    # Final analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL ABLATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    acc_a = tester.results['A_price_only']['scores']['accuracy']\n",
    "    acc_b = tester.results['B_onchain_only']['scores']['accuracy']\n",
    "    acc_c = tester.results['C_hybrid']['scores']['accuracy']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Accuracy Comparison:\")\n",
    "    print(f\"  Model A (price):    {acc_a:.2%}\")\n",
    "    print(f\"  Model B (on-chain): {acc_b:.2%} ({acc_b - 0.5:.2%} above random)\")\n",
    "    print(f\"  Model C (hybrid):   {acc_c:.2%}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Incremental Value:\")\n",
    "    c_vs_a = acc_c - acc_a\n",
    "    print(f\"  On-chain adds: {c_vs_a:+.2%} accuracy\")\n",
    "    \n",
    "    if c_vs_a > 0.01:\n",
    "        print(f\"  âœ… PASS: On-chain signals add {c_vs_a:.2%} value\")\n",
    "    elif c_vs_a > 0:\n",
    "        print(f\"  âš ï¸ MARGINAL: Only {c_vs_a:.2%} improvement\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAIL: On-chain adds no value (or hurts)\")\n",
    "    \n",
    "    # ROC AUC comparison\n",
    "    auc_c = tester.results['C_hybrid']['scores']['roc_auc']\n",
    "    auc_a = tester.results['A_price_only']['scores']['roc_auc']\n",
    "    print(f\"\\nðŸ“ˆ ROC AUC:\")\n",
    "    print(f\"  Model C: {auc_c:.4f} vs Model A: {auc_a:.4f} ({auc_c - auc_a:+.4f})\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Next Steps:\")\n",
    "    if c_vs_a > 0.01:\n",
    "        print(\"  â€¢ On-chain signals validated\")\n",
    "        print(\"  â€¢ Proceed to feature importance analysis\")\n",
    "        print(\"  â€¢ Test with RandomForest/XGBoost for non-linear effects\")\n",
    "    else:\n",
    "        print(\"  â€¢ On-chain signals may be regime-specific\")\n",
    "        print(\"  â€¢ Try interaction features (price_vol * whale_flow)\")\n",
    "        print(\"  â€¢ Consider threshold-based rules instead of ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446a2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading merged dataset...\n",
      "âœ… Loaded 1097 rows, 36 columns\n",
      "\n",
      "âš™ï¸ Engineering features...\n",
      "âœ… Features created: 58 columns\n",
      "Date range: 2022-12-24 â†’ 2025-12-24\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: BASELINE TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL A: A Price Only\n",
      "======================================================================\n",
      "Features: 20\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1077\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.2%\n",
      "  Class 1 (up):   51.8%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5564\n",
      "  Precision: 0.5525\n",
      "  Recall:    0.8343\n",
      "  F1 Score:  0.6620\n",
      "  ROC AUC:   0.5721\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: ON-CHAIN SIGNAL TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL B: B Onchain Only\n",
      "======================================================================\n",
      "Features: 27\n",
      "Feature list: deposit_tx_count, withdrawal_tx_count, deposit_withdrawal_ratio, exchange_volume_ratio, exchange_flow_share...\n",
      "Clean samples: 1086\n",
      "Date range: 2023-01-02 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 47.9%\n",
      "  Class 1 (up):   52.1%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5193\n",
      "  Precision: 0.5588\n",
      "  Recall:    0.4697\n",
      "  F1 Score:  0.4879\n",
      "  ROC AUC:   0.5389\n",
      "\n",
      "======================================================================\n",
      "PHASE 3: HYBRID MODEL (PRICE + ON-CHAIN)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL C: C Hybrid\n",
      "======================================================================\n",
      "Features: 47\n",
      "Feature list: eth_daily_return, eth_log_return, eth_vol7, eth_vol30, eth_rsi...\n",
      "Clean samples: 1075\n",
      "Date range: 2023-01-13 â†’ 2025-12-24\n",
      "\n",
      "Target distribution:\n",
      "  Class 0 (down): 48.3%\n",
      "  Class 1 (up):   51.7%\n",
      "\n",
      "Running 5-fold walk-forward CV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Accuracy:  0.5397\n",
      "  Precision: 0.5736\n",
      "  Recall:    0.6838\n",
      "  F1 Score:  0.5839\n",
      "  ROC AUC:   0.5707\n",
      "\n",
      "======================================================================\n",
      "ABLATION COMPARISON\n",
      "======================================================================\n",
      "                n_features  accuracy  precision    recall        f1   roc_auc\n",
      "A_price_only            20  0.556425   0.552493  0.834288  0.661993  0.572111\n",
      "B_onchain_only          27  0.519337   0.558822  0.469713  0.487912  0.538859\n",
      "C_hybrid                47  0.539665   0.573636  0.683770  0.583876  0.570721\n",
      "\n",
      "ðŸ† Best Accuracy: A_price_only (0.5564)\n",
      "ðŸ† Best F1 Score: A_price_only (0.6620)\n",
      "\n",
      "======================================================================\n",
      "FINAL ABLATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Accuracy Comparison:\n",
      "  Model A (price):    55.64%\n",
      "  Model B (on-chain): 51.93% (1.93% above random)\n",
      "  Model C (hybrid):   53.97%\n",
      "\n",
      "ðŸŽ¯ Incremental Value:\n",
      "  On-chain adds: -1.68% accuracy\n",
      "  âŒ FAIL: On-chain adds no value (or hurts)\n",
      "\n",
      "ðŸ“ˆ ROC AUC:\n",
      "  Model C: 0.5707 vs Model A: 0.5721 (-0.0014)\n",
      "\n",
      "ðŸ’¡ Next Steps:\n",
      "  â€¢ On-chain signals may be regime-specific\n",
      "  â€¢ Try interaction features (price_vol * whale_flow)\n",
      "  â€¢ Consider threshold-based rules instead of ML\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: FEATURE DOMINANCE CHECK\n",
      "======================================================================\n",
      "Analyzing what Model C actually learned...\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE: C_HYBRID\n",
      "======================================================================\n",
      "\n",
      "ðŸŒ² Training RandomForest for feature importance...\n",
      "\n",
      "ðŸ“Š TOP 15 FEATURES (RandomForest):\n",
      "               feature  importance\n",
      "        eth_log_return    0.039125\n",
      "      eth_daily_return    0.038299\n",
      "      btc_daily_return    0.037194\n",
      "        btc_log_return    0.034549\n",
      "             btc_vol30    0.029228\n",
      "          eth_ret_lag7    0.028104\n",
      "               eth_rsi    0.027319\n",
      "   exchange_flow_share    0.026649\n",
      "tx_per_active_delta_1d    0.026565\n",
      "          btc_ret_lag7    0.024723\n",
      "              btc_vol7    0.024104\n",
      "   whale_tx_zscore_90d    0.024094\n",
      "             eth_vol30    0.023844\n",
      "   eth_burned_delta_1d    0.023138\n",
      "     eth_btc_ratio_ma7    0.022574\n",
      "\n",
      "ðŸ” Feature Breakdown (Top 15):\n",
      "  Price/Technical: 12\n",
      "  Whale/On-chain:  4\n",
      "\n",
      "âœ… GOOD: Whale features present in top 15:\n",
      "     exchange_flow_share: 0.0266\n",
      "     tx_per_active_delta_1d: 0.0266\n",
      "     whale_tx_zscore_90d: 0.0241\n",
      "     eth_burned_delta_1d: 0.0231\n",
      "\n",
      "ðŸ”€ Computing permutation importance (slower but accurate)...\n",
      "\n",
      "ðŸ“Š TOP 15 FEATURES (Permutation):\n",
      "                      feature  importance\n",
      "                 eth_ret_lag1    0.016651\n",
      "           eth_outperformance    0.014419\n",
      "  whale_volume_ratio_delta_3d    0.013116\n",
      "          median_gas_delta_7d    0.012093\n",
      "                 btc_ret_lag1    0.011256\n",
      "                    btc_vol30    0.010326\n",
      "       tx_per_active_delta_1d    0.010047\n",
      "        exchange_volume_ratio    0.010047\n",
      "          whale_tx_zscore_90d    0.009581\n",
      "          eth_burned_delta_1d    0.009395\n",
      "smart_contract_ratio_delta_1d    0.009209\n",
      "                     btc_vol7    0.009116\n",
      "  whale_volume_ratio_delta_1d    0.008744\n",
      "                 net_flow_ma7    0.008744\n",
      "             eth_btc_corr_30d    0.008558\n",
      "\n",
      "======================================================================\n",
      "FINAL VERDICT\n",
      "======================================================================\n",
      "\n",
      "Whale features in top 10:\n",
      "  RandomForest:  2/10\n",
      "  Permutation:   5/10\n",
      "\n",
      "âœ… VERDICT: Whale data actively used\n",
      "   â†’ Performance issue may be feature engineering, not signal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: ABLATION TESTING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "class AblationTester:\n",
    "    \"\"\"Test feature group contributions via ablation study\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='next_day_price_direction'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.results = {}\n",
    "        \n",
    "    def define_feature_groups(self):\n",
    "        \"\"\"Define Model A-C feature sets\"\"\"\n",
    "        \n",
    "        # Model A: Price-only baseline\n",
    "        self.groups = {\n",
    "            'A_price_only': [\n",
    "                'eth_daily_return', 'eth_log_return', 'eth_vol7', 'eth_vol30',\n",
    "                'eth_rsi', 'eth_ret_lag1', 'eth_ret_lag3', 'eth_ret_lag7',\n",
    "                'btc_daily_return', 'btc_log_return', 'btc_vol7', 'btc_vol30',\n",
    "                'btc_rsi', 'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
    "                'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d',\n",
    "                'eth_outperformance'\n",
    "            ],\n",
    "            \n",
    "            # Model B: On-chain only\n",
    "            'B_onchain_only': [\n",
    "                'deposit_tx_count', 'withdrawal_tx_count', 'deposit_withdrawal_ratio',\n",
    "                'exchange_volume_ratio', 'exchange_flow_share', 'net_exchange_flow_ratio',\n",
    "                'whale_exchange_deposits_eth', 'whale_exchange_withdrawals_eth',\n",
    "                'whale_net_exchange_flow_eth', 'whale_exchange_flow_ratio',\n",
    "                'whale_exchange_asymmetry', 'whale_tx_count', 'whale_volume_eth',\n",
    "                'whale_volume_ratio', 'whale_volume_ratio_delta_1d',\n",
    "                'whale_volume_ratio_delta_3d', 'whale_tx_zscore_90d',\n",
    "                'mega_whale_ratio', 'net_flow_ma7', 'tx_per_active_delta_1d',\n",
    "                'tx_per_active_zscore_90d', 'block_fullness_delta_1d',\n",
    "                'eth_burned_delta_1d', 'eth_burned_zscore_90d', 'median_gas_delta_1d',\n",
    "                'median_gas_delta_7d', 'smart_contract_ratio_delta_1d'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Model C: Hybrid (A + B, no raw prices)\n",
    "        self.groups['C_hybrid'] = (\n",
    "            self.groups['A_price_only'] + \n",
    "            self.groups['B_onchain_only']\n",
    "        )\n",
    "        \n",
    "        # Verify features exist for all groups\n",
    "        for group_name, features in self.groups.items():\n",
    "            missing = [f for f in features if f not in self.df.columns]\n",
    "            if missing:\n",
    "                print(f\"âš ï¸ {group_name} missing: {missing}\")\n",
    "                self.groups[group_name] = [f for f in features if f in self.df.columns]\n",
    "        \n",
    "        return self.groups\n",
    "    \n",
    "    def prepare_data(self, features):\n",
    "        \"\"\"Clean data for modeling\"\"\"\n",
    "        df_clean = self.df[features + [self.target_col, 'block_date']].copy()\n",
    "        \n",
    "        # Remove rows with missing target or features\n",
    "        df_clean = df_clean.dropna(subset=[self.target_col])\n",
    "        df_clean = df_clean.dropna(subset=features)\n",
    "        \n",
    "        # Sort by date\n",
    "        df_clean = df_clean.sort_values('block_date').reset_index(drop=True)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def time_series_cv(self, X, y, n_splits=5):\n",
    "        \"\"\"Walk-forward time series cross-validation\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        scores = {\n",
    "            'accuracy': [], 'precision': [], 'recall': [], \n",
    "            'f1': [], 'roc_auc': []\n",
    "        }\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "            scores['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "            scores['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "            scores['roc_auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "    \n",
    "    def run_ablation(self, model_name='A_price_only', n_splits=5):\n",
    "        \"\"\"Execute ablation test for a feature group\"\"\"\n",
    "        \n",
    "        features = self.groups[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MODEL {model_name.split('_')[0]}: {model_name.replace('_', ' ').title()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Features: {len(features)}\")\n",
    "        print(f\"Feature list: {', '.join(features[:5])}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = self.prepare_data(features)\n",
    "        print(f\"Clean samples: {len(df_clean)}\")\n",
    "        print(f\"Date range: {df_clean['block_date'].min().date()} â†’ \"\n",
    "              f\"{df_clean['block_date'].max().date()}\")\n",
    "        \n",
    "        X = df_clean[features]\n",
    "        y = df_clean[self.target_col]\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = y.value_counts(normalize=True)\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        print(f\"  Class 0 (down): {class_dist[0]:.1%}\")\n",
    "        print(f\"  Class 1 (up):   {class_dist[1]:.1%}\")\n",
    "        \n",
    "        # Run CV\n",
    "        print(f\"\\nRunning {n_splits}-fold walk-forward CV...\")\n",
    "        scores = self.time_series_cv(X, y, n_splits)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'features': features,\n",
    "            'n_features': len(features),\n",
    "            'n_samples': len(df_clean),\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nðŸ“Š RESULTS:\")\n",
    "        print(f\"  Accuracy:  {scores['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {scores['f1']:.4f}\")\n",
    "        print(f\"  ROC AUC:   {scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def feature_importance_analysis(self, model_name='C_hybrid'):\n",
    "        \"\"\"Analyze which features drive predictions\"\"\"\n",
    "        \n",
    "        if model_name not in self.results:\n",
    "            print(f\"âŒ Model {model_name} not tested yet\")\n",
    "            return\n",
    "        \n",
    "        features = self.groups[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FEATURE IMPORTANCE: {model_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = self.prepare_data(features)\n",
    "        X = df_clean[features]\n",
    "        y = df_clean[self.target_col]\n",
    "        \n",
    "        # Train on full dataset (for importance, not prediction)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Train RandomForest for better feature importance\n",
    "        print(\"\\nðŸŒ² Training RandomForest for feature importance...\")\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=8, \n",
    "                                     random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_scaled, y)\n",
    "        \n",
    "        # Feature importance from RF\n",
    "        feat_imp = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š TOP 15 FEATURES (RandomForest):\")\n",
    "        print(feat_imp.head(15).to_string(index=False))\n",
    "        \n",
    "        # Categorize features\n",
    "        price_feats = [f for f in feat_imp['feature'].values[:15] \n",
    "                       if any(x in f for x in ['eth_', 'btc_', 'ratio', 'return', \n",
    "                                                'vol', 'rsi', 'lag', 'outperform'])]\n",
    "        whale_feats = [f for f in feat_imp['feature'].values[:15] \n",
    "                       if any(x in f for x in ['whale', 'exchange', 'deposit', \n",
    "                                                'withdrawal', 'flow', 'tx_', 'gas', \n",
    "                                                'burned', 'mega'])]\n",
    "        \n",
    "        print(f\"\\nðŸ” Feature Breakdown (Top 15):\")\n",
    "        print(f\"  Price/Technical: {len(price_feats)}\")\n",
    "        print(f\"  Whale/On-chain:  {len(whale_feats)}\")\n",
    "        \n",
    "        if len(whale_feats) >= 3:\n",
    "            print(f\"\\nâœ… GOOD: Whale features present in top 15:\")\n",
    "            for f in whale_feats:\n",
    "                imp = feat_imp[feat_imp['feature'] == f]['importance'].values[0]\n",
    "                print(f\"     {f}: {imp:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ WARNING: Only {len(whale_feats)} whale features in top 15\")\n",
    "            print(\"  â†’ Model relies almost entirely on price momentum\")\n",
    "        \n",
    "        # Permutation importance (more reliable)\n",
    "        print(f\"\\nðŸ”€ Computing permutation importance (slower but accurate)...\")\n",
    "        perm_imp = permutation_importance(rf, X_scaled, y, n_repeats=10, \n",
    "                                          random_state=42, n_jobs=-1)\n",
    "        \n",
    "        perm_df = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': perm_imp.importances_mean\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š TOP 15 FEATURES (Permutation):\")\n",
    "        print(perm_df.head(15).to_string(index=False))\n",
    "        \n",
    "        return feat_imp, perm_df\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all tested models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models tested yet!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ABLATION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            name: res['scores'] for name, res in self.results.items()\n",
    "        }).T\n",
    "        \n",
    "        comparison['n_features'] = [res['n_features'] for res in self.results.values()]\n",
    "        comparison = comparison[['n_features', 'accuracy', 'precision', \n",
    "                                 'recall', 'f1', 'roc_auc']]\n",
    "        \n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = comparison['accuracy'].idxmax()\n",
    "        best_f1 = comparison['f1'].idxmax()\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Accuracy: {best_acc} ({comparison.loc[best_acc, 'accuracy']:.4f})\")\n",
    "        print(f\"ðŸ† Best F1 Score: {best_f1} ({comparison.loc[best_f1, 'f1']:.4f})\")\n",
    "        \n",
    "        return comparison\n",
    "        \"\"\"Compare all tested models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No models tested yet!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ABLATION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            name: res['scores'] for name, res in self.results.items()\n",
    "        }).T\n",
    "        \n",
    "        comparison['n_features'] = [res['n_features'] for res in self.results.values()]\n",
    "        comparison = comparison[['n_features', 'accuracy', 'precision', \n",
    "                                 'recall', 'f1', 'roc_auc']]\n",
    "        \n",
    "        print(comparison.to_string())\n",
    "        \n",
    "        # Find best model\n",
    "        best_acc = comparison['accuracy'].idxmax()\n",
    "        best_f1 = comparison['f1'].idxmax()\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Accuracy: {best_acc} ({comparison.loc[best_acc, 'accuracy']:.4f})\")\n",
    "        print(f\"ðŸ† Best F1 Score: {best_f1} ({comparison.loc[best_f1, 'f1']:.4f})\")\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target: next day price direction\"\"\"\n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load merged dataset\n",
    "    print(\"ðŸ“‚ Loading merged dataset...\")\n",
    "    df_merged = pd.read_csv('merged_ml_dataset.csv')\n",
    "    df_merged['block_date'] = pd.to_datetime(df_merged['block_date'], utc=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df_merged)} rows, {len(df_merged.columns)} columns\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\nâš™ï¸ Engineering features...\")\n",
    "    df_merged = add_price_features(df_merged, 'eth_price', 'eth')\n",
    "    df_merged = add_price_features(df_merged, 'btc_price', 'btc')\n",
    "    df_merged = add_correlation_features(df_merged)\n",
    "    df_merged = create_target(df_merged)\n",
    "    \n",
    "    print(f\"âœ… Features created: {len(df_merged.columns)} columns\")\n",
    "    print(f\"Date range: {df_merged['block_date'].min().date()} â†’ \"\n",
    "          f\"{df_merged['block_date'].max().date()}\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = AblationTester(df_merged)\n",
    "    \n",
    "    # Define feature groups\n",
    "    tester.define_feature_groups()\n",
    "    \n",
    "    # Run Model A: Price-only baseline\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: BASELINE TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('A_price_only', n_splits=5)\n",
    "    \n",
    "    # Run Model B: On-chain only\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: ON-CHAIN SIGNAL TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('B_onchain_only', n_splits=5)\n",
    "    \n",
    "    # Run Model C: Hybrid\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 3: HYBRID MODEL (PRICE + ON-CHAIN)\")\n",
    "    print(\"=\"*70)\n",
    "    tester.run_ablation('C_hybrid', n_splits=5)\n",
    "    \n",
    "    # Compare results\n",
    "    comparison = tester.compare_models()\n",
    "    \n",
    "    # Final analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL ABLATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    acc_a = tester.results['A_price_only']['scores']['accuracy']\n",
    "    acc_b = tester.results['B_onchain_only']['scores']['accuracy']\n",
    "    acc_c = tester.results['C_hybrid']['scores']['accuracy']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Accuracy Comparison:\")\n",
    "    print(f\"  Model A (price):    {acc_a:.2%}\")\n",
    "    print(f\"  Model B (on-chain): {acc_b:.2%} ({acc_b - 0.5:.2%} above random)\")\n",
    "    print(f\"  Model C (hybrid):   {acc_c:.2%}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Incremental Value:\")\n",
    "    c_vs_a = acc_c - acc_a\n",
    "    print(f\"  On-chain adds: {c_vs_a:+.2%} accuracy\")\n",
    "    \n",
    "    if c_vs_a > 0.01:\n",
    "        print(f\"  âœ… PASS: On-chain signals add {c_vs_a:.2%} value\")\n",
    "    elif c_vs_a > 0:\n",
    "        print(f\"  âš ï¸ MARGINAL: Only {c_vs_a:.2%} improvement\")\n",
    "    else:\n",
    "        print(f\"  âŒ FAIL: On-chain adds no value (or hurts)\")\n",
    "    \n",
    "    # ROC AUC comparison\n",
    "    auc_c = tester.results['C_hybrid']['scores']['roc_auc']\n",
    "    auc_a = tester.results['A_price_only']['scores']['roc_auc']\n",
    "    print(f\"\\nðŸ“ˆ ROC AUC:\")\n",
    "    print(f\"  Model C: {auc_c:.4f} vs Model A: {auc_a:.4f} ({auc_c - auc_a:+.4f})\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Next Steps:\")\n",
    "    if c_vs_a > 0.01:\n",
    "        print(\"  â€¢ On-chain signals validated\")\n",
    "        print(\"  â€¢ Proceed to feature importance analysis\")\n",
    "        print(\"  â€¢ Test with RandomForest/XGBoost for non-linear effects\")\n",
    "    else:\n",
    "        print(\"  â€¢ On-chain signals may be regime-specific\")\n",
    "        print(\"  â€¢ Try interaction features (price_vol * whale_flow)\")\n",
    "        print(\"  â€¢ Consider threshold-based rules instead of ML\")\n",
    "    \n",
    "    # PHASE 2: Feature Importance Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: FEATURE DOMINANCE CHECK\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Analyzing what Model C actually learned...\")\n",
    "    \n",
    "    feat_imp_rf, feat_imp_perm = tester.feature_importance_analysis('C_hybrid')\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL VERDICT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    whale_in_top10_rf = sum(1 for f in feat_imp_rf['feature'].values[:10] \n",
    "                            if any(x in f for x in ['whale', 'exchange', 'deposit', \n",
    "                                                     'withdrawal', 'flow', 'tx_', \n",
    "                                                     'burned', 'mega']))\n",
    "    \n",
    "    whale_in_top10_perm = sum(1 for f in feat_imp_perm['feature'].values[:10] \n",
    "                              if any(x in f for x in ['whale', 'exchange', 'deposit', \n",
    "                                                       'withdrawal', 'flow', 'tx_', \n",
    "                                                       'burned', 'mega']))\n",
    "    \n",
    "    print(f\"\\nWhale features in top 10:\")\n",
    "    print(f\"  RandomForest:  {whale_in_top10_rf}/10\")\n",
    "    print(f\"  Permutation:   {whale_in_top10_perm}/10\")\n",
    "    \n",
    "    if whale_in_top10_perm == 0:\n",
    "        print(\"\\nâŒ VERDICT: Model ignores whale data entirely\")\n",
    "        print(\"   â†’ Whale signals redundant with price momentum\")\n",
    "    elif whale_in_top10_perm <= 2:\n",
    "        print(\"\\nâš ï¸ VERDICT: Whale data plays minor role\")\n",
    "        print(\"   â†’ Try regime-specific modeling or interactions\")\n",
    "    else:\n",
    "        print(\"\\nâœ… VERDICT: Whale data actively used\")\n",
    "        print(\"   â†’ Performance issue may be feature engineering, not signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277d6fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED ML PIPELINE - PHASE 3 & 4\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Loading data...\n",
      "âœ… Loaded 1097 rows, 36 columns\n",
      "   Date range: 2022-12-24 â†’ 2025-12-24\n",
      "\n",
      "âš™ï¸ Creating basic features...\n",
      "\n",
      "ðŸ”„ Phase 3: Creating enhanced features...\n",
      "  â€¢ Relative price features...\n",
      "  â€¢ Gated momentum features...\n",
      "  â€¢ Regime features...\n",
      "  â€¢ Interaction features...\n",
      "  âœ… Phase 3 complete!\n",
      "\n",
      "ðŸ”„ Phase 4: Creating confidence-weighted target...\n",
      "  âœ… Median move: 0.0120\n",
      "  âœ… High-conf samples: 548\n",
      "\n",
      "âœ… Feature engineering complete: 79 columns\n",
      "\n",
      "ðŸ” Verifying features...\n",
      "  âœ… D_relative_price: 26 features available\n",
      "  âœ… E_gated_hybrid: 23 features available\n",
      "\n",
      "======================================================================\n",
      "MODEL D: RELATIVE PRICE BASELINE\n",
      "======================================================================\n",
      "\n",
      "All Samples:     Acc=0.5390, AUC=0.5573\n",
      "High-Confidence: Acc=0.5455, AUC=0.5721\n",
      "Lift:            +0.65%\n",
      "\n",
      "======================================================================\n",
      "MODEL E: GATED HYBRID (THE FIX)\n",
      "======================================================================\n",
      "\n",
      "All Samples:     Acc=0.5311, AUC=0.5307\n",
      "High-Confidence: Acc=0.5545, AUC=0.5416\n",
      "Lift:            +2.35%\n",
      "\n",
      "======================================================================\n",
      "FINAL VERDICT\n",
      "======================================================================\n",
      "\n",
      "High-Confidence Accuracy:\n",
      "  Model D (Relative Price): 54.55%\n",
      "  Model E (Gated Hybrid):   55.45%\n",
      "\n",
      "ðŸŽ¯ ON-CHAIN VALUE: +0.91%\n",
      "\n",
      "âš ï¸ MARGINAL: Small improvement detected\n",
      "   â†’ May be regime-specific\n",
      "   â†’ Try splitting by market conditions\n",
      "\n",
      "======================================================================\n",
      "COMPARISON TABLE\n",
      "======================================================================\n",
      "                  Model D   Model E\n",
      "All Samples      0.538983  0.531073\n",
      "High-Confidence  0.545455  0.554545\n",
      "Lift             0.006471  0.023472\n",
      "\n",
      "ðŸ’¾ Save enhanced dataset? Uncomment below:\n",
      "# df.to_csv('enhanced_ml_dataset.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Enhanced ML Pipeline - Phase 3 & 4 Fixes\n",
    "Run this script directly on your merged_ml_dataset.csv\n",
    "\n",
    "This implements:\n",
    "- Phase 3: Relative price features + gated momentum + interactions\n",
    "- Phase 4: Confidence-weighted evaluation\n",
    "\n",
    "No external imports needed - everything is self-contained.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: BASIC FEATURE ENGINEERING (from your original code)\n",
    "# ============================================================================\n",
    "\n",
    "def add_price_features(df, price_col, prefix):\n",
    "    \"\"\"Add price-based ML features\"\"\"\n",
    "    df = df.sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    df[f'{prefix}_daily_return'] = df[price_col].pct_change()\n",
    "    df[f'{prefix}_log_return'] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    df[f'{prefix}_vol7'] = df[f'{prefix}_daily_return'].rolling(7, min_periods=1).std()\n",
    "    df[f'{prefix}_vol30'] = df[f'{prefix}_daily_return'].rolling(30, min_periods=1).std()\n",
    "    \n",
    "    # RSI\n",
    "    returns = df[f'{prefix}_daily_return']\n",
    "    gains = returns.where(returns > 0, 0).rolling(14, min_periods=1).mean()\n",
    "    losses = -returns.where(returns < 0, 0).rolling(14, min_periods=1).mean()\n",
    "    rs = gains / (losses + 1e-10)\n",
    "    df[f'{prefix}_rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Lags\n",
    "    for lag in [1, 3, 7]:\n",
    "        df[f'{prefix}_ret_lag{lag}'] = df[f'{prefix}_daily_return'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_correlation_features(df):\n",
    "    \"\"\"Add ETH-BTC correlation features\"\"\"\n",
    "    df['eth_btc_ratio'] = df['eth_price'] / df['btc_price']\n",
    "    df['eth_btc_ratio_ma7'] = df['eth_btc_ratio'].rolling(7, min_periods=1).mean()\n",
    "    df['eth_btc_corr_30d'] = df['eth_daily_return'].rolling(30, min_periods=20).corr(df['btc_daily_return'])\n",
    "    df['eth_outperformance'] = df['eth_daily_return'] - df['btc_daily_return']\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PHASE 3 ENHANCEMENTS - RELATIVE FEATURES & GATING\n",
    "# ============================================================================\n",
    "\n",
    "def add_phase3_features(df):\n",
    "    \"\"\"Add Phase 3 enhancements: relative prices, gated momentum, interactions\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”„ Phase 3: Creating enhanced features...\")\n",
    "    \n",
    "    # 1. RELATIVE PRICE FEATURES (replace absolute prices)\n",
    "    print(\"  â€¢ Relative price features...\")\n",
    "    for col in ['eth_price', 'btc_price']:\n",
    "        prefix = col.split('_')[0]\n",
    "        \n",
    "        # Z-scores\n",
    "        mean_90d = df[col].rolling(90, min_periods=30).mean()\n",
    "        std_90d = df[col].rolling(90, min_periods=30).std()\n",
    "        df[f'{prefix}_price_zscore_90d'] = (df[col] - mean_90d) / (std_90d + 1e-10)\n",
    "        \n",
    "        # Distance from MAs\n",
    "        ma_20 = df[col].rolling(20, min_periods=10).mean()\n",
    "        df[f'{prefix}_pct_from_ma20'] = (df[col] - ma_20) / (ma_20 + 1e-10)\n",
    "        \n",
    "        ma_50 = df[col].rolling(50, min_periods=20).mean()\n",
    "        df[f'{prefix}_pct_from_ma50'] = (df[col] - ma_50) / (ma_50 + 1e-10)\n",
    "    \n",
    "    # 2. GATED MOMENTUM (the key innovation!)\n",
    "    print(\"  â€¢ Gated momentum features...\")\n",
    "    df['eth_momentum_valid'] = df['eth_ret_lag1'] * np.sign(df['whale_net_exchange_flow_eth'])\n",
    "    df['btc_momentum_valid'] = df['btc_ret_lag1'] * np.sign(df['net_exchange_flow_ratio'])\n",
    "    \n",
    "    vol_threshold = df['eth_vol7'].quantile(0.5)\n",
    "    df['eth_momentum_lowvol'] = df['eth_ret_lag1'] * (df['eth_vol7'] < vol_threshold).astype(float)\n",
    "    \n",
    "    df['whale_confirms_price'] = (\n",
    "        np.sign(df['eth_ret_lag1']) == np.sign(df['whale_net_exchange_flow_eth'])\n",
    "    ).astype(float)\n",
    "    \n",
    "    volume_z = (df['whale_volume_eth'] - df['whale_volume_eth'].rolling(30).mean()) / \\\n",
    "               (df['whale_volume_eth'].rolling(30).std() + 1e-10)\n",
    "    df['momentum_volume_confirmed'] = df['eth_ret_lag1'] * (volume_z > 0.5).astype(float)\n",
    "    \n",
    "    # 3. REGIME FEATURES\n",
    "    print(\"  â€¢ Regime features...\")\n",
    "    vol_75th = df['eth_vol30'].quantile(0.75)\n",
    "    df['high_vol_regime'] = (df['eth_vol30'] > vol_75th).astype(float)\n",
    "    \n",
    "    df['trend_strength'] = abs(df['eth_price'].rolling(20).mean() - df['eth_price'].rolling(50).mean())\n",
    "    trend_25th = df['trend_strength'].quantile(0.25)\n",
    "    df['choppy_regime'] = (df['trend_strength'] < trend_25th).astype(float)\n",
    "    \n",
    "    corr_median = df['eth_btc_corr_30d'].median()\n",
    "    df['low_corr_regime'] = (df['eth_btc_corr_30d'] < corr_median).astype(float)\n",
    "    \n",
    "    # 4. INTERACTIONS\n",
    "    print(\"  â€¢ Interaction features...\")\n",
    "    df['vol_x_whale_flow'] = df['eth_vol7'] * df['whale_net_exchange_flow_eth']\n",
    "    df['momentum_x_exchange_pressure'] = df['eth_ret_lag1'] * df['net_exchange_flow_ratio']\n",
    "    df['whale_activity_x_vol'] = df['whale_tx_count'] * df['eth_vol30']\n",
    "    df['gas_x_momentum'] = df['median_gas_delta_1d'] * df['eth_ret_lag1']\n",
    "    \n",
    "    print(\"  âœ… Phase 3 complete!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: PHASE 4 - CONFIDENCE-WEIGHTED TARGET\n",
    "# ============================================================================\n",
    "\n",
    "def add_phase4_target(df):\n",
    "    \"\"\"Add Phase 4: confidence-weighted target\"\"\"\n",
    "    print(\"\\nðŸ”„ Phase 4: Creating confidence-weighted target...\")\n",
    "    \n",
    "    df['next_day_return'] = df['eth_price'].pct_change().shift(-1)\n",
    "    df['next_day_price_direction'] = (df['next_day_return'] > 0).astype(int)\n",
    "    df['signal_confidence'] = abs(df['next_day_return'])\n",
    "    \n",
    "    confidence_median = df['signal_confidence'].median()\n",
    "    df['high_confidence_sample'] = (df['signal_confidence'] > confidence_median)\n",
    "    \n",
    "    print(f\"  âœ… Median move: {confidence_median:.4f}\")\n",
    "    print(f\"  âœ… High-conf samples: {df['high_confidence_sample'].sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: FEATURE GROUPS\n",
    "# ============================================================================\n",
    "\n",
    "def define_feature_groups():\n",
    "    \"\"\"Define enhanced feature groups\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'D_relative_price': [\n",
    "            'eth_price_zscore_90d', 'eth_pct_from_ma20', 'eth_pct_from_ma50',\n",
    "            'btc_price_zscore_90d', 'btc_pct_from_ma20', 'btc_pct_from_ma50',\n",
    "            'eth_daily_return', 'eth_log_return', 'eth_vol7', 'eth_vol30',\n",
    "            'btc_daily_return', 'btc_log_return', 'btc_vol7', 'btc_vol30',\n",
    "            'eth_rsi', 'btc_rsi',\n",
    "            'eth_ret_lag1', 'eth_ret_lag3', 'eth_ret_lag7',\n",
    "            'btc_ret_lag1', 'btc_ret_lag3', 'btc_ret_lag7',\n",
    "            'eth_btc_ratio', 'eth_btc_ratio_ma7', 'eth_btc_corr_30d', 'eth_outperformance'\n",
    "        ],\n",
    "        \n",
    "        'E_gated_hybrid': [\n",
    "            'eth_price_zscore_90d', 'eth_pct_from_ma20',\n",
    "            'btc_price_zscore_90d', 'btc_pct_from_ma20',\n",
    "            'eth_momentum_valid', 'btc_momentum_valid', 'eth_momentum_lowvol',\n",
    "            'whale_confirms_price', 'momentum_volume_confirmed',\n",
    "            'high_vol_regime', 'choppy_regime', 'low_corr_regime',\n",
    "            'vol_x_whale_flow', 'momentum_x_exchange_pressure',\n",
    "            'whale_activity_x_vol', 'gas_x_momentum',\n",
    "            'whale_net_exchange_flow_eth', 'whale_tx_zscore_90d',\n",
    "            'whale_volume_ratio_delta_3d', 'exchange_flow_share',\n",
    "            'tx_per_active_delta_1d', 'eth_burned_zscore_90d', 'median_gas_delta_7d'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: ENHANCED ABLATION TESTER\n",
    "# ============================================================================\n",
    "\n",
    "def run_confidence_weighted_cv(df, features, n_splits=5):\n",
    "    \"\"\"Run CV with confidence-weighted evaluation\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    required = features + ['next_day_price_direction', 'signal_confidence', 'block_date']\n",
    "    df_clean = df[required].dropna().sort_values('block_date').reset_index(drop=True)\n",
    "    \n",
    "    X = df_clean[features]\n",
    "    y = df_clean['next_day_price_direction']\n",
    "    conf = df_clean['signal_confidence']\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    scores_all = {'acc': [], 'prec': [], 'rec': [], 'f1': [], 'auc': []}\n",
    "    scores_hc = {'acc': [], 'prec': [], 'rec': [], 'f1': [], 'auc': []}\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        conf_test = conf.iloc[test_idx]\n",
    "        \n",
    "        # Scale and train\n",
    "        scaler = StandardScaler()\n",
    "        X_train_sc = scaler.fit_transform(X_train)\n",
    "        X_test_sc = scaler.transform(X_test)\n",
    "        \n",
    "        model = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "        model.fit(X_train_sc, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_sc)\n",
    "        y_proba = model.predict_proba(X_test_sc)[:, 1]\n",
    "        \n",
    "        # All samples\n",
    "        scores_all['acc'].append(accuracy_score(y_test, y_pred))\n",
    "        scores_all['prec'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        scores_all['rec'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        scores_all['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        scores_all['auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        # High confidence only\n",
    "        hc_mask = conf_test > conf_test.median()\n",
    "        if hc_mask.sum() > 10:\n",
    "            y_test_hc = y_test[hc_mask]\n",
    "            y_pred_hc = y_pred[hc_mask]\n",
    "            y_proba_hc = y_proba[hc_mask]\n",
    "            \n",
    "            scores_hc['acc'].append(accuracy_score(y_test_hc, y_pred_hc))\n",
    "            scores_hc['prec'].append(precision_score(y_test_hc, y_pred_hc, zero_division=0))\n",
    "            scores_hc['rec'].append(recall_score(y_test_hc, y_pred_hc, zero_division=0))\n",
    "            scores_hc['f1'].append(f1_score(y_test_hc, y_pred_hc, zero_division=0))\n",
    "            scores_hc['auc'].append(roc_auc_score(y_test_hc, y_proba_hc))\n",
    "    \n",
    "    return {\n",
    "        'all': {k: np.mean(v) for k, v in scores_all.items()},\n",
    "        'hc': {k: np.mean(v) for k, v in scores_hc.items()},\n",
    "        'n_samples': len(df_clean)\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ENHANCED ML PIPELINE - PHASE 3 & 4\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\nðŸ“‚ Loading data...\")\n",
    "    df = pd.read_csv('merged_ml_dataset.csv')\n",
    "    df['block_date'] = pd.to_datetime(df['block_date'], utc=True)\n",
    "    print(f\"âœ… Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    print(f\"   Date range: {df['block_date'].min().date()} â†’ {df['block_date'].max().date()}\")\n",
    "    \n",
    "    # 2. Add basic features\n",
    "    print(\"\\nâš™ï¸ Creating basic features...\")\n",
    "    df = add_price_features(df, 'eth_price', 'eth')\n",
    "    df = add_price_features(df, 'btc_price', 'btc')\n",
    "    df = add_correlation_features(df)\n",
    "    \n",
    "    # 3. Add Phase 3 enhancements\n",
    "    df = add_phase3_features(df)\n",
    "    \n",
    "    # 4. Add Phase 4 target\n",
    "    df = add_phase4_target(df)\n",
    "    \n",
    "    print(f\"\\nâœ… Feature engineering complete: {len(df.columns)} columns\")\n",
    "    \n",
    "    # 5. Define feature groups\n",
    "    groups = define_feature_groups()\n",
    "    \n",
    "    # Verify features exist\n",
    "    print(\"\\nðŸ” Verifying features...\")\n",
    "    for name, feats in groups.items():\n",
    "        missing = [f for f in feats if f not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"  âš ï¸ {name} missing {len(missing)} features: {missing[:3]}...\")\n",
    "            groups[name] = [f for f in feats if f in df.columns]\n",
    "        print(f\"  âœ… {name}: {len(groups[name])} features available\")\n",
    "    \n",
    "    # 6. Run ablation tests\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL D: RELATIVE PRICE BASELINE\")\n",
    "    print(\"=\"*70)\n",
    "    results['D'] = run_confidence_weighted_cv(df, groups['D_relative_price'])\n",
    "    print(f\"\\nAll Samples:     Acc={results['D']['all']['acc']:.4f}, AUC={results['D']['all']['auc']:.4f}\")\n",
    "    print(f\"High-Confidence: Acc={results['D']['hc']['acc']:.4f}, AUC={results['D']['hc']['auc']:.4f}\")\n",
    "    print(f\"Lift:            {(results['D']['hc']['acc'] - results['D']['all']['acc'])*100:+.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL E: GATED HYBRID (THE FIX)\")\n",
    "    print(\"=\"*70)\n",
    "    results['E'] = run_confidence_weighted_cv(df, groups['E_gated_hybrid'])\n",
    "    print(f\"\\nAll Samples:     Acc={results['E']['all']['acc']:.4f}, AUC={results['E']['all']['auc']:.4f}\")\n",
    "    print(f\"High-Confidence: Acc={results['E']['hc']['acc']:.4f}, AUC={results['E']['hc']['auc']:.4f}\")\n",
    "    print(f\"Lift:            {(results['E']['hc']['acc'] - results['E']['all']['acc'])*100:+.2f}%\")\n",
    "    \n",
    "    # 7. Final comparison\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL VERDICT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    d_hc = results['D']['hc']['acc']\n",
    "    e_hc = results['E']['hc']['acc']\n",
    "    value = e_hc - d_hc\n",
    "    \n",
    "    print(f\"\\nHigh-Confidence Accuracy:\")\n",
    "    print(f\"  Model D (Relative Price): {d_hc:.2%}\")\n",
    "    print(f\"  Model E (Gated Hybrid):   {e_hc:.2%}\")\n",
    "    print(f\"\\nðŸŽ¯ ON-CHAIN VALUE: {value:+.2%}\")\n",
    "    \n",
    "    if value > 0.02:\n",
    "        print(\"\\nâœ… SUCCESS! On-chain signals add significant value\")\n",
    "        print(\"   â†’ Proceed with Model E for production\")\n",
    "        print(\"   â†’ Use high-confidence filtering for trading\")\n",
    "    elif value > 0:\n",
    "        print(\"\\nâš ï¸ MARGINAL: Small improvement detected\")\n",
    "        print(\"   â†’ May be regime-specific\")\n",
    "        print(\"   â†’ Try splitting by market conditions\")\n",
    "    else:\n",
    "        print(\"\\nâŒ FAILURE: Gating didn't solve the problem\")\n",
    "        print(\"   â†’ On-chain may lag price\")\n",
    "        print(\"   â†’ Try leading indicators or regime-split models\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Model D': [results['D']['all']['acc'], results['D']['hc']['acc'], \n",
    "                    results['D']['hc']['acc'] - results['D']['all']['acc']],\n",
    "        'Model E': [results['E']['all']['acc'], results['E']['hc']['acc'],\n",
    "                    results['E']['hc']['acc'] - results['E']['all']['acc']]\n",
    "    }, index=['All Samples', 'High-Confidence', 'Lift'])\n",
    "    \n",
    "    print(comparison.to_string())\n",
    "    \n",
    "    print(\"\\nðŸ’¾ Save enhanced dataset? Uncomment below:\")\n",
    "    print(\"# df.to_csv('enhanced_ml_dataset.csv', index=False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
